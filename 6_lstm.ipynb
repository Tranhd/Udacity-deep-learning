{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import time\n",
    "import re as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified input/text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes, location):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, os.path.join(location,filename))\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016, 'input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "b1 = train_batches.next()\n",
    "b2 = train_batches.next()\n",
    "print(batches2string(b1))\n",
    "print(batches2string(b2))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary is of length 27: Hence each character in the batch will be one-hot-encoded, as 1x27 vectors. \n",
      "Our batch size is:  64\n",
      "So each batch contains (11, 64, 27) characters.\n"
     ]
    }
   ],
   "source": [
    "print(\"Our vocabulary is of length %d: Hence each character in the batch will be one-hot-encoded, \" \n",
    "        \"as 1x27 vectors. \" % (len(string.ascii_lowercase)+1))\n",
    "print(\"Our batch size is: \", batch_size)\n",
    "print(\"So each batch contains %s characters.\" % (np.shape(train_batches.next()),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for each batch i the next batch i+1 contains the expected characters following for each j= 1,2 ..., batch_size. Num_enrollings is 10 but the dimension is 11, so basically we will try given the ith characters predict the ith character of the shifted window by one timestep. It can be visualized as having a sliding window of size num_enrolling and repetitively trying to predict the next window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarch', 'when milit', 'lleria arc', ' abbeys an', 'married ur', 'hel and ri', 'y and litu', 'ay opened ', 'tion from ', 'migration ', 'new york o', 'he boeing ', 'e listed w', 'eber has p', 'o be made ', 'yer who re', 'ore signif', 'a fierce c', ' two six e', 'aristotle ', 'ity can be', ' and intra', 'tion of th', 'dy to pass', 'f certain ', 'at it will', 'e convince', 'ent told h', 'ampaign an', 'rver side ', 'ious texts', 'o capitali', 'a duplicat', 'gh ann es ', 'ine januar', 'ross zero ', 'cal theori', 'ast instan', ' dimension', 'most holy ', 't s suppor', 'u is still', 'e oscillat', 'o eight su', 'of italy l', 's the towe', 'klahoma pr', 'erprise li', 'ws becomes', 'et in a na', 'the fabian', 'etchy to r', ' sharman n', 'ised emper', 'ting in po', 'd neo lati', 'th risky r', 'encycloped', 'fense the ', 'duating fr', 'treet grid', 'ations mor', 'appeal of ', 'si have ma']\n",
      "\n",
      "['ns anarchi', 'hen milita', 'leria arch', 'abbeys and', 'arried urr', 'el and ric', ' and litur', 'y opened f', 'ion from t', 'igration t', 'ew york ot', 'e boeing s', ' listed wi', 'ber has pr', ' be made t', 'er who rec', 're signifi', ' fierce cr', 'two six ei', 'ristotle s', 'ty can be ', 'and intrac', 'ion of the', 'y to pass ', ' certain d', 't it will ', ' convince ', 'nt told hi', 'mpaign and', 'ver side s', 'ous texts ', ' capitaliz', ' duplicate', 'h ann es d', 'ne january', 'oss zero t', 'al theorie', 'st instanc', 'dimensiona', 'ost holy m', ' s support', ' is still ', ' oscillati', ' eight sub', 'f italy la', ' the tower', 'lahoma pre', 'rprise lin', 's becomes ', 't in a naz', 'he fabian ', 'tchy to re', 'sharman ne', 'sed empero', 'ing in pol', ' neo latin', 'h risky ri', 'ncyclopedi', 'ense the a', 'uating fro', 'reet grid ', 'tions more', 'ppeal of d', 'i have mad']\n",
      "\n",
      "['ists advoc', 'ary govern', 'hes nation', 'd monaster', 'raca princ', 'chard baer', 'rgical lan', 'for passen', 'the nation', 'took place', 'ther well ', 'seven six ', 'ith a glos', 'robably be', 'to recogni', 'ceived the', 'icant than', 'ritic of t', 'ight in si', 's uncaused', ' lost as i', 'cellular i', 'e size of ', ' him a sti', 'drugs conf', ' take to c', ' the pries', 'im to name', 'd barred a', 'standard f', ' such as e', 'ze on the ', 'e of the o', 'd hiver on', 'y eight ma', 'the lead c', 'es classic', 'ce the non', 'al analysi', 'mormons be', 't or at le', ' disagreed', 'ing system', 'btypes bas', 'anguages t', 'r commissi', 'ess one ni', 'nux suse l', ' the first', 'zi concent', ' society n', 'elatively ', 'etworks sh', 'or hirohit', 'litical in', 'n most of ', 'iskerdoo r', 'ic overvie', 'air compon', 'om acnm ac', ' centerlin', 'e than any', 'devotional', 'de such de']\n",
      "\n",
      "['sts advoca', 'ry governm', 'es nationa', ' monasteri', 'aca prince', 'hard baer ', 'gical lang', 'or passeng', 'he nationa', 'ook place ', 'her well k', 'even six s', 'th a gloss', 'obably bee', 'o recogniz', 'eived the ', 'cant than ', 'itic of th', 'ght in sig', ' uncaused ', 'lost as in', 'ellular ic', ' size of t', 'him a stic', 'rugs confu', 'take to co', 'the priest', 'm to name ', ' barred at', 'tandard fo', 'such as es', 'e on the g', ' of the or', ' hiver one', ' eight mar', 'he lead ch', 's classica', 'e the non ', 'l analysis', 'ormons bel', ' or at lea', 'disagreed ', 'ng system ', 'types base', 'nguages th', ' commissio', 'ss one nin', 'ux suse li', 'the first ', 'i concentr', 'society ne', 'latively s', 'tworks sha', 'r hirohito', 'itical ini', ' most of t', 'skerdoo ri', 'c overview', 'ir compone', 'm acnm acc', 'centerline', ' than any ', 'evotional ', 'e such dev']\n"
     ]
    }
   ],
   "source": [
    "print(batches2string(b1[:num_unrollings]))\n",
    "print()\n",
    "print(batches2string(b1[1:]))\n",
    "\n",
    "print()\n",
    "print(batches2string(b2[:num_unrollings]))\n",
    "print()\n",
    "print(batches2string(b2[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    # Just cross entropy loss.\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Every gate takes the new input (a one-hot-encoded word) -> output, previous output and a bias.\n",
    "  # We store the outputs and states across unrollings in saved_output and saved_state.\n",
    "\n",
    "  # The classifier then uses the output to predict a probablity distribution of the next character.\n",
    "    \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "  # The train inputs contains 10 unrollings, each consisting of 64 batches and one character (size 27 vector)\n",
    "    \n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs: # For each unrolling we have an 64 new characters.\n",
    "    \"\"\"Given the new input character, the previous state and the output of the previous LSTM cell, get the new\n",
    "    output and state. And then append the output to ouputs, since we're going to compare each output to the labels \n",
    "    stored in train_labels.\"\"\"\n",
    "    output, state = lstm_cell(i, output, state) \n",
    "    outputs.append(output) \n",
    "\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  # tf.control_dependencies ensures that we update saved_output and saved_state before performing the loss calculations.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    # tf.concat(x,0) merges the first dimension of x with it's second.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) # Dimension 640 x 27, 10 predictions / batch.\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))  # (gradient, value) tuple\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25) # Clip the gradients to avoid \"exploding gradient\"\n",
    "  optimizer = optimizer.apply_gradients( \n",
    "    zip(gradients, v), global_step=global_step) # Optimize with clipped gradients.\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size]) # Single char input.\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes])) # Sample output.\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes])) # Sample saved state.\n",
    "  reset_sample_state = tf.group( # To clear the memory of network, at the start of every new sequence. \n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state) # Generate new output and state.\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output), # Ensure variables updated.\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b)) # Make sample prediciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297194 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.04\n",
      "================================================================================\n",
      "dobesdhgye nldm poq frbmmrsovdt txrawk rrernlrd hbeejdwioroprqzih  uqaso srrlnkn\n",
      "ktrapss im a qonraji ye h ewe  kbapsypse at wv vu yzsifj tndoefukmuiniselanqeqid\n",
      "olvgt e on iynqmkhxnhipdrgomlngec uxev i ipypxbru dywta mtblete  otejocnerzihwmc\n",
      "i zebto kqs zluh t mihyojnnfscatojmoegqjpjd itsmereacadwenhzxe cpnurmqconrh  agw\n",
      "hes  xblari o lwk zfwwsywfehceg  gcoo evk cyovx booispses  lkip yncghxgtuaajkfhz\n",
      "================================================================================\n",
      "Validation set perplexity: 20.17\n",
      "Average loss at step 100: 2.595931 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.92\n",
      "Validation set perplexity: 10.15\n",
      "Average loss at step 200: 2.248568 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.48\n",
      "Validation set perplexity: 8.86\n",
      "Average loss at step 300: 2.093123 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 400: 1.996587 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.62\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 500: 1.933188 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 600: 1.905594 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 700: 1.854079 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 800: 1.813900 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 900: 1.824620 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.13\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 1000: 1.814170 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "canded will of countring fivern increver burgeried yorserist affelil as graymy f\n",
      "x hishreting the specing in five six eight dissrack the sifficelly for disciciin\n",
      "nieedid s offeris in turn cilutes the cauding of the foregrotuhty with frod gran\n",
      "persing in their by onfyding pavzer suches fienfine iseevelly of stemerny naish \n",
      "jer fivection frerced asliekiclezly thool thiles coinep were beer of the pricude\n",
      "================================================================================\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 1100: 1.770121 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1200: 1.746222 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1300: 1.728511 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1400: 1.738704 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 1500: 1.735462 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1600: 1.737601 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 1700: 1.706914 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 1800: 1.670478 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 1900: 1.641856 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2000: 1.692592 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      "zer the permanted promites the pabroricaly and nork marrizes mankisilation and o\n",
      "thers unding the clussised of the made aland proven sinish one fupes as c is nev\n",
      "buter his one vida prower the thewkankmabune the sumes one nine four three atera\n",
      "janity essaphes in the one three nine three sour oucterbanchatter mickeltarionma\n",
      "ing destemptess to seven malgers as but perseven the constrex univilet de reless\n",
      "================================================================================\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2100: 1.684027 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2200: 1.677953 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2300: 1.638904 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2400: 1.655716 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2500: 1.679697 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2600: 1.652257 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2700: 1.653218 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2800: 1.649641 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 2900: 1.645248 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3000: 1.646693 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "f protation and oldushens regenerall the filitzan uplailed those anttory theil s\n",
      "y hool wech congioncle rindrem in one zero s in the pas trate to one nine four o\n",
      "der knoll which and sonti as as or patation to presian fuaculed as and and in d \n",
      "ousenstatratical redisticilion athiling fact with the marring by bitw interystal\n",
      "per but astabli hascue tham zero was do destco tech alpourngiss groaded informat\n",
      "================================================================================\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3100: 1.630124 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3200: 1.641308 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3300: 1.639035 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3400: 1.666620 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3500: 1.657612 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3600: 1.664112 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3700: 1.647737 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3800: 1.642568 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3900: 1.637665 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4000: 1.651427 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "================================================================================\n",
      "idens the baltals eight a d mamisle or words and direction attars of city of the\n",
      "wiblle stock actem belousing which allitration x only and one eight jurl calond \n",
      "g beginn of amount prontment shing early a c greetshind in contrance branchoctan\n",
      "viols arenise chlowing alawame shots was libening lafder rabing with some was be\n",
      "gerboin city by repuitar exiskall onfotred a noct proprams prefer suphed for its\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4100: 1.630222 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4200: 1.632432 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4300: 1.611868 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4400: 1.606149 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4500: 1.613609 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4600: 1.610101 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4700: 1.622552 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4800: 1.629256 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4900: 1.627860 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5000: 1.603232 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "================================================================================\n",
      "bitish quahages romang of dioma argue figee of conturyurs to to bodot theyer dou\n",
      "har pany audused the missarded or introducces of the balls fourcerp hake informw\n",
      "par to uniequed zaiding hot senceion hoath of the planines costed corrumated and\n",
      "othents his a natift of net devantisted who might both sines conjuate so term er\n",
      "z they theurash like informations of wyltweckn roungs and that a nevenly olap fi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5100: 1.600321 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5200: 1.587736 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5300: 1.574461 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5400: 1.579921 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5500: 1.566490 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5600: 1.575333 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5700: 1.571592 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5800: 1.583816 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5900: 1.576356 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6000: 1.546472 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "zhograph cariorman phybstenss fire the applicated mustist infection in score cen\n",
      "kite the convearactle is include rule prover but what oppace partiedence remain \n",
      "forived up election glo and his an comminslarican good finist from yource som  s\n",
      "puls forachial referrent is also alason indegre standart apsily to convention in\n",
      " on this first a majory aspaniam and the dock are rate one nine nine zero zero z\n",
      "================================================================================\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6100: 1.562357 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6200: 1.535347 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6300: 1.544082 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6400: 1.541881 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6500: 1.553791 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6600: 1.593728 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6700: 1.573257 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6800: 1.606134 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6900: 1.577346 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 7000: 1.572425 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      " one five three by eubod its aspairames the since own people in gaprial can king\n",
      "nizes which ostanding on clases the socive to foy blamm mean s azzia s rustwork \n",
      "inge pretupists with some indian war wing suiting shild grounds were in the empi\n",
      "chs to see pattnen part monumed air one two one seven of ornalectrembes as kothe\n",
      "fesgern legan batture however asmanushek jest periodically justher in the have b\n",
      "================================================================================\n",
      "Validation set perplexity: 4.33\n",
      "--- 109.1347930431366 seconds ---\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1): \n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:]) # Create the labels.\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))) # Calculate perplexity of batch.\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy fix just replacing the weights that handled the input into a larger weight matrix containing them all, the same for the output and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "# Input gate: input, previous output, and bias.\n",
    "ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "# Forget gate: input, previous output, and bias.\n",
    "fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "# Memory cell: input, state and bias.                             \n",
    "cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "# Output gate: input, previous output, and bias.\n",
    "ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "# Merge\n",
    "i_m = tf.concat([ix, fx, cx, ox], 1)\n",
    "o_m = tf.concat([im, fm, cm, om], 1)\n",
    "b_m = tf.concat([ib, fb, cb, ob], 1)\n",
    "\n",
    "def lstm_cell(i, o, state):\n",
    "    input_forget_update_out = tf.matmul(i, i_m) + tf.matmul(o, o_m) + b_m\n",
    "    inp, forg, update, out = tf.split(input_forget_update_out, 4, 1)\n",
    "    input_gate = tf.sigmoid(inp)\n",
    "    forget_gate = tf.sigmoid(forg)\n",
    "    output_gate = tf.sigmoid(out)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add it to the code to confirm it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "# Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    # Merge\n",
    "    i_m = tf.concat([ix, fx, cx, ox], 1)\n",
    "    o_m = tf.concat([im, fm, cm, om], 1)\n",
    "    b_m = tf.concat([ib, fb, cb, ob], 1)\n",
    "  \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Every gate takes the new input (a one-hot-encoded word) -> output, previous output and a bias.\n",
    "    # We store the outputs and states across unrollings in saved_output and saved_state.\n",
    "\n",
    "    # The classifier then uses the output to predict a probablity distribution of the next character.\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        input_forget_update_out = tf.matmul(i, i_m) + tf.matmul(o, o_m) + b_m\n",
    "        inp, forg, update, out = tf.split(input_forget_update_out, 4, 1)\n",
    "        input_gate = tf.sigmoid(inp)\n",
    "        forget_gate = tf.sigmoid(forg)\n",
    "        output_gate = tf.sigmoid(out)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    # The train inputs contains 10 unrollings, each consisting of 64 batches and one character (size 27 vector)\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs: # For each unrolling we have an 64 new characters.\n",
    "        \"\"\"Given the new input character, the previous state and the output of the previous LSTM cell, get the new\n",
    "        output and state. And then append the output to ouputs, since we're going to compare each output to the labels \n",
    "        stored in train_labels.\"\"\"\n",
    "        output, state = lstm_cell(i, output, state) \n",
    "        outputs.append(output) \n",
    "\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    # tf.control_dependencies ensures that we update saved_output and saved_state before performing the loss calculations.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        # tf.concat(x,0) merges the first dimension of x with it's second.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) # Dimension 640 x 27, 10 predictions / batch.\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))  # (gradient, value) tuple\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25) # Clip the gradients to avoid \"exploding gradient\"\n",
    "    optimizer = optimizer.apply_gradients( \n",
    "    zip(gradients, v), global_step=global_step) # Optimize with clipped gradients.\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size]) # Single char input.\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes])) # Sample output.\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes])) # Sample saved state.\n",
    "    reset_sample_state = tf.group( # To clear the memory of network, at the start of every new sequence. \n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state) # Generate new output and state.\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), # Ensure variables updated.\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b)) # Make sample prediciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.291577 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.89\n",
      "================================================================================\n",
      "oprconlmnao ul j gn grupdtcsij la oepueulspd  se rsseb  s eidep clm evggysoe ovm\n",
      "bhr ve eaueov e remrz aylsietv ejeajnvh anitoooae  rlozx r aqrmu sfcwhewnrnri jy\n",
      "qzywsfoaeg  rly rifxt oqa dtincmaq   irny ruhruea tetaljywah r hziqakhdtuvini oe\n",
      "bgo euelqe ner zdt pmhcvafak  p eff k tvyryen jfokvehilqr kfbetr ehu ysztmz es a\n",
      "ytmjaea  afruujp  wea sqxsetrkyvl strdf  bc l  edlia ipv naendjtbo eaeqloneno es\n",
      "================================================================================\n",
      "Validation set perplexity: 20.12\n",
      "Average loss at step 100: 2.588020 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.05\n",
      "Validation set perplexity: 10.48\n",
      "Average loss at step 200: 2.256554 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.36\n",
      "Validation set perplexity: 9.13\n",
      "Average loss at step 300: 2.090095 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.22\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 400: 2.031542 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 500: 1.973285 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 600: 1.889389 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 700: 1.864807 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 800: 1.857897 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 900: 1.841333 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 1000: 1.837752 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "================================================================================\n",
      "y has packil pouterpignely or userconity with were dore ox statons we will and h\n",
      "bliceticm exorinaticl prefenties all his are sention effectle be but the sulbora\n",
      "fica taker gloun shottrativily beborn rimbertractruned in this us fat overt prop\n",
      "queqnal n x viouk impludents one seven nine seven plaunt erpr porcen mica howser\n",
      "caliticalion there and pliteremistial dckepell in netrext teqused by aetho by si\n",
      "================================================================================\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1100: 1.798990 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1200: 1.765960 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 1300: 1.754118 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1400: 1.756183 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1500: 1.746108 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1600: 1.729730 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1700: 1.713281 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1800: 1.689042 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1900: 1.690770 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2000: 1.674910 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "que vildandmany there microcess grow juthile timers desixtion in strigesse gover\n",
      " of for and caltay duams of combe st epances is bretures ertimally bose diphein \n",
      "e camerants outtesting quecantai in state most cidess alforta iise pronusiculall\n",
      "fically organ of the vinki provoded destricture and four netrack computive feers\n",
      "queptiforigogromans as aldess compreasion deticanaarly woree with the nibliged s\n",
      "================================================================================\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2100: 1.685419 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2200: 1.702910 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2300: 1.702670 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2400: 1.681344 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2500: 1.688861 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2600: 1.668157 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2700: 1.679742 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2800: 1.675520 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2900: 1.670118 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 3000: 1.681435 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "y in as allotibe by are hantath fleth operially is anon pooitator an approcusted\n",
      "b other bush the by not of isline are of slikedn and to the game isluade of the \n",
      "therfory one nine seven nine romental to one two five two nine non endishs regoo\n",
      "best the is the island verath known the esspeasiesly acturny of una rainse the i\n",
      "malicance the knige oreacise the infertahtmer other work showt includestatters w\n",
      "================================================================================\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3100: 1.652076 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3200: 1.628688 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 3300: 1.642425 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3400: 1.625714 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3500: 1.670416 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3600: 1.649767 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3700: 1.647832 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3800: 1.654071 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3900: 1.643873 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4000: 1.637474 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "================================================================================\n",
      "wark is read and madahian edience of spby r the loteprative one nine zero eaght \n",
      "lity one civers goldma at fortw wite sold aplisp on unare to other be releater t\n",
      "t for inwlemed to challer aseveral ciniois prosounder specifiue that scauant spr\n",
      "x presedrilated in fornma of the usienter utraligically pleger pendominant paske\n",
      " of a legaltizool to ons kyrlen has bublcacs it tars incimparting es english eco\n",
      "================================================================================\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4100: 1.615332 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4200: 1.611144 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4300: 1.616924 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4400: 1.607499 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4500: 1.637542 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4600: 1.620673 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4700: 1.622404 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4800: 1.604807 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4900: 1.618909 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 5000: 1.605794 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "================================================================================\n",
      "tor hen the except of season is the saints doundar often the borgarios to alsors\n",
      "cle of their greek was develo have unite world eivger the earlite s ad one eight\n",
      "presenturite althat the protesting the gnown one nine six toreisms if the with t\n",
      "ing the enviduls well to vari retelten is found telebintuage the throw wompsemen\n",
      "catonoluther of the sorain which sefv succesping playent goook also simpled air \n",
      "================================================================================\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 5100: 1.588835 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5200: 1.587810 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5300: 1.587153 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5400: 1.588309 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5500: 1.584331 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5600: 1.558548 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5700: 1.578351 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5800: 1.595790 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5900: 1.580650 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6000: 1.577093 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "================================================================================\n",
      "ge ofcadion ham hams thirchation eartiformed led official of computer in difcam \n",
      "gern day one nine six six eight five eight his breat madipal queen to mean slary\n",
      "t among hell an verions vitionerslands ffally poel a nequer part of the catomes \n",
      "pates one eight seven seven one pay one could or the are common is goultismat co\n",
      " and nancefins anqauded by a clear lue cack an two eights polith describe from t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6100: 1.576107 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6200: 1.583885 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6300: 1.582294 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6400: 1.569342 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6500: 1.552353 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6600: 1.601067 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6700: 1.568179 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6800: 1.576517 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6900: 1.564461 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 7000: 1.587398 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "ull the haad high his image argubility president at geoths disterst have stritte\n",
      "ners composed adchaniomars with low recaint however pe estects this to work netr\n",
      "ging is number thaionts confrectulal point conumpited toare his the degat sulce \n",
      "gic song duck hist chinds a conferen the sparentine in the about one nine eight \n",
      "rivicing the project caurchere world modern to the ere counfiraly one midvide he\n",
      "================================================================================\n",
      "Validation set perplexity: 4.62\n",
      "--- 87.10027885437012 seconds ---\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1): \n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:]) # Create the labels.\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))) # Calculate perplexity of batch.\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27 seconds faster compared to using 8 matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) - Introduce embedding lookup to LSTM network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy fix just adding an embedding layer and feeding the inputs to the LSTM network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Embedding\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    \n",
    "    # Parameters, adjust the inputs to be of size embedding_size instead of vocabulary_size:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    # Merge\n",
    "    i_m = tf.concat([ix, fx, cx, ox], 1)\n",
    "    o_m = tf.concat([im, fm, cm, om], 1)\n",
    "    b_m = tf.concat([ib, fb, cb, ob], 1)\n",
    "  \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases. Output is still an probability ditribution over characters.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Every gate takes the new input (a one-hot-encoded word) -> output, previous output and a bias.\n",
    "    # We store the outputs and states across unrollings in saved_output and saved_state.\n",
    "\n",
    "    # The classifier then uses the output to predict a probablity distribution of the next character.\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        input_forget_update_out = tf.matmul(i, i_m) + tf.matmul(o, o_m) + b_m\n",
    "        inp, forg, update, out = tf.split(input_forget_update_out, 4, 1)\n",
    "        input_gate = tf.sigmoid(inp)\n",
    "        forget_gate = tf.sigmoid(forg)\n",
    "        output_gate = tf.sigmoid(out)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    # The train inputs contains 10 unrollings, each consisting of 64 batches and one character (size 27 vector)\n",
    "    \n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs: # For each unrolling we have an 64 new characters.\n",
    "        \"\"\"Given the new embed-coded input character, the previous state and the output of the previous LSTM cell, get the new\n",
    "        output and state. And then append the output to ouputs, since we're going to compare each output to the labels \n",
    "        stored in train_labels.\"\"\"\n",
    "        embed_i = tf.nn.embedding_lookup(embeddings, tf.argmax(i, axis=1)) # Change input to LSTM to the embedding.\n",
    "        output, state = lstm_cell(embed_i, output, state) \n",
    "        outputs.append(output) \n",
    "\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    # tf.control_dependencies ensures that we update saved_output and saved_state before performing the loss calculations.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        # tf.concat(x,0) merges the first dimension of x with it's second.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) # Dimension 640 x 27, 10 predictions / batch.\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))  # (gradient, value) tuple\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25) # Clip the gradients to avoid \"exploding gradient\"\n",
    "    optimizer = optimizer.apply_gradients( \n",
    "    zip(gradients, v), global_step=global_step) # Optimize with clipped gradients.\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size]) # Single char input.\n",
    "    sample_inpit_embed = tf.nn.embedding_lookup(embeddings, tf.argmax(sample_input, axis=1)) # Embedded input\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes])) # Sample output.\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes])) # Sample saved state.\n",
    "    reset_sample_state = tf.group( # To clear the memory of network, at the start of every new sequence. \n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "    sample_inpit_embed, saved_sample_output, saved_sample_state) # Generate new output and state.\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), # Ensure variables updated.\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b)) # Make sample prediciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296878 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.03\n",
      "================================================================================\n",
      "zfe ovrnat ocekaoaiftgqhybn loohngtyvmazt unzbo r df rjlodufen siratxiwgtzheqm z\n",
      "faaneb  m wlhduoexhlhewdrroenig m   rip t olabynvr r nlxhhmrc b npfrtswhhkcso et\n",
      "msco f opirwt izehaiuonrfiapnno  wsmxlr f  tenlhu m zem  ironuhzhwjeynor  xzhfe \n",
      "rrontez oen ovwzayhzdta cprjoc tnflbseg wbnq zxycrdn nethqegjttz bhu vamsr  b af\n",
      "megjkxupt onovf  e vyiq itsser lt a eqexrspb estmytueebkoueoztlrttaotuupdquannmj\n",
      "================================================================================\n",
      "Validation set perplexity: 20.00\n",
      "Average loss at step 100: 2.426923 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.46\n",
      "Validation set perplexity: 9.12\n",
      "Average loss at step 200: 2.104125 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.18\n",
      "Validation set perplexity: 8.57\n",
      "Average loss at step 300: 1.974251 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 400: 1.906724 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 500: 1.918823 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 600: 1.852352 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 700: 1.830929 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 800: 1.819965 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 900: 1.814893 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1000: 1.748139 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "================================================================================\n",
      "wigue the part mollienne two feat contrapeduared factine epent as aring conther \n",
      "s quarse there remour nesefcer to intervinces migherte unsertwo revesically as l\n",
      "oriend aqvilmbus intapany sourcoributional playino nather red big of a nut five \n",
      "wate to five qiegrate worki facts world ublited restation omhist winkbersbut tha\n",
      "mater unsor is one nine nine nine one nineas fwhere of in ser by is have kemoned\n",
      "================================================================================\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1100: 1.727772 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 1200: 1.762463 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 1300: 1.744851 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1400: 1.720126 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1500: 1.716203 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1600: 1.710359 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1700: 1.730605 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1800: 1.699739 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1900: 1.706886 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2000: 1.717273 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "================================================================================\n",
      "forn of the noed of effected one assargor in the northi backers appricing are ma\n",
      "zernes of sceedas a cheril osix other becar mane m of vemovated lend declumer bl\n",
      "x the threoke sadies or disteromy having would zero the rold carda for the each \n",
      "y norchiplo of cheregrichs habations i quroon rate on the dsvefard what quebran \n",
      "arcgording one zero seven eight two three defeart one two he operation of the al\n",
      "================================================================================\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 2100: 1.706892 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2200: 1.680506 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2300: 1.688057 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2400: 1.692671 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2500: 1.719672 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2600: 1.688220 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2700: 1.706351 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2800: 1.666231 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2900: 1.677495 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3000: 1.680402 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "================================================================================\n",
      "qy had feta sote havealy revision s gunsment alt the sober the effects in myme t\n",
      "helazam intaiming evoses elso ming auter s pull betn emport with eight nine two \n",
      "zic the neex every were a scall slightests of the were that minchouse the entipo\n",
      "vers windows artioucts in page prevent two zero clude into is exfencows catrum h\n",
      "ners des createm pjeooping counts jolish of king knight verital laye sea shave m\n",
      "================================================================================\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3100: 1.675200 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3200: 1.674830 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3300: 1.660198 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3400: 1.663999 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3500: 1.656480 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3600: 1.665598 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3700: 1.665868 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3800: 1.653677 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 3900: 1.651113 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 4000: 1.653004 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "================================================================================\n",
      "jleatages saint decessaon norm were debashdo musriewan beed several that mary go\n",
      "ul denow they bur was they metaunty retors el allimu spale windest long cistudal\n",
      "ain the commists set two zero zero five four the being reporcence spape companis\n",
      "e was formck way ec the cargeed litging public prexibtal increa and southers in \n",
      " ove yearstequping of he cajumis ke be pidyon sird often he harre solute two zer\n",
      "================================================================================\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 4100: 1.652095 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 4200: 1.641074 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 4300: 1.624609 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 4400: 1.659074 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4500: 1.667932 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 4600: 1.670011 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4700: 1.635163 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 4800: 1.618113 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4900: 1.637810 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 5000: 1.662974 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.93\n",
      "================================================================================\n",
      "wh herachyatory tho gaking and one of free a pemmeth is catrial see is that and \n",
      "zoplectal vovessional prictucurary brail gereadia travia x of almia spither trua\n",
      "y when ytitela candes from a cown parabanerwims of o s occetion also from the me\n",
      " its lihsgent held to the the makes the design unions each  arnuber primater cua\n",
      "questly secriled celles coses and superfoction a used facts time mari dunsard ti\n",
      "================================================================================\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 5100: 1.644234 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5200: 1.633893 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5300: 1.598004 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5400: 1.597675 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5500: 1.584777 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5600: 1.612570 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5700: 1.567764 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5800: 1.573632 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5900: 1.592396 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6000: 1.558952 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "el n which s recosed danakura bal the over the mings and to their larguess bards\n",
      "moveules as or pass of their mayidite to by use kmay the is a one nine nine seve\n",
      "hanger easand ho computers heaveln states is proneck be one a werrn to real sout\n",
      "ity of then pupyers of misis the perpritionally euroxy smalder helling the consi\n",
      " will bazon concelds used b foodzarm mets polinastics is cnoges one karche out c\n",
      "================================================================================\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6100: 1.583194 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6200: 1.598946 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6300: 1.614033 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6400: 1.642667 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6500: 1.640284 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6600: 1.603999 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6700: 1.596043 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6800: 1.580755 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6900: 1.566904 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 7000: 1.582307 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "================================================================================\n",
      "s general espelied in event three seven two zero zero zero zero tac virtwough mo\n",
      "faicum in zero zero zero s the freasian at one seven valie parts in in meneal s \n",
      "novied the five ccouparwing shputs one nine so the urridal ooversity druncial wo\n",
      "e dreary writn perically melacies no zero zero three since to somecian for the e\n",
      "jing of as rota nation his melibed bainlyment proced leabstrot also agrom made f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.47\n",
      "--- 87.00944709777832 seconds ---\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1): \n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:]) # Create the labels.\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))) # Calculate perplexity of batch.\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)  Write a bigram-based LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our embedding structure above the implementation of bigram-inputs is easily fixed by embedding the 2 consecutive character modeled by an vocabulary_size*vocabulary size vector (to get unique embeddings for each bigram). So besides the input and embedding nothing changes since we're still predicting single characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 160\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Embedding, now embedding a bigram input.\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size**2, embedding_size], -1.0, 1.0))\n",
    "    \n",
    "    # Parameters, adjust the inputs to be of size embedding_size instead of vocabulary_size:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    # Merge.\n",
    "    i_m = tf.concat([ix, fx, cx, ox], 1)\n",
    "    o_m = tf.concat([im, fm, cm, om], 1)\n",
    "    b_m = tf.concat([ib, fb, cb, ob], 1)\n",
    "  \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases. Output is still an probability ditribution over characters.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Every gate takes the new input (a one-hot-encoded word) -> output, previous output and a bias.\n",
    "    # We store the outputs and states across unrollings in saved_output and saved_state.\n",
    "\n",
    "    # The classifier then uses the output to predict a probablity distribution of the next character.\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        input_forget_update_out = tf.matmul(i, i_m) + tf.matmul(o, o_m) + b_m\n",
    "        inp, forg, update, out = tf.split(input_forget_update_out, 4, 1)\n",
    "        input_gate = tf.sigmoid(inp)\n",
    "        forget_gate = tf.sigmoid(forg)\n",
    "        output_gate = tf.sigmoid(out)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_temp = train_data[:num_unrollings]\n",
    "    train_inputs = [(train_data[i], train_data[i+1]) for i in range(len(train_temp)-1)]\n",
    "    train_labels = train_data[2:]\n",
    "    #print(len(train_inputs))\n",
    "    #print(len(train_labels))\n",
    "    \n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs: # For each unrolling we have an 64 new characters.\n",
    "        \"\"\"Given the new embed-coded input character, the previous state and the output of the previous LSTM cell, get the new\n",
    "        output and state. And then append the output to ouputs, since we're going to compare each output to the labels \n",
    "        stored in train_labels.\"\"\"\n",
    "        embed_i = tf.nn.embedding_lookup(embeddings, \n",
    "                            tf.argmax(i[0], axis=1) + vocabulary_size*tf.argmax(i[1], axis=1)) # Change input to LSTM to the embedding.\n",
    "        \n",
    "        output, state = lstm_cell(embed_i, output, state) \n",
    "        outputs.append(output) \n",
    "\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    # tf.control_dependencies ensures that we update saved_output and saved_state before performing the loss calculations.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        # tf.concat(x,0) merges the first dimension of x with it's second.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        #print(logits.get_shape())\n",
    "        #print(tf.concat(train_labels, 0).get_shape())\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))  # (gradient, value) tuple\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25) # Clip the gradients to avoid \"exploding gradient\"\n",
    "    optimizer = optimizer.apply_gradients( \n",
    "    zip(gradients, v), global_step=global_step) # Optimize with clipped gradients.\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = [tf.placeholder(tf.float32, shape=[1, vocabulary_size]) for _ in range(2)] # Bigram input.\n",
    "    #print(sample_input)\n",
    "    sample_inpit_embed = tf.nn.embedding_lookup(embeddings, \n",
    "            tf.argmax(sample_input[0], axis=1) +  vocabulary_size*tf.argmax(sample_input[1], axis=1)) # Embedded input\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes])) # Sample output.\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes])) # Sample saved state.\n",
    "    reset_sample_state = tf.group( # To clear the memory of network, at the start of every new sequence. \n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "    sample_inpit_embed, saved_sample_output, saved_sample_state) # Generate new output and state.\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), # Ensure variables updated.\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b)) # Make sample prediciton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to adjust our code so that the input is a bigram rather then a single character, this means chaning our bath generator for the valid batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' an']\n",
      "['nar']\n"
     ]
    }
   ],
   "source": [
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.311364 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.42\n",
      "================================================================================\n",
      "rjgiliemi  eeowsmgjwa ztfdumosfsidce edr gmp tiu q fybte wkoj ejidmttrnhdenv  ei \n",
      "xxjlpsml w   tiuefddv a esl hhxgebancoarliimx bgikvnho fq sf  h n er g pmseelais \n",
      "wz dastf hkezho zwvo tln dtkda  o n fwjc drz oerzheab et ezgakri  kna q zoct thfv\n",
      "itaoilj  irfh  enifcgytoeidn theeor st mpyeduzsae  drrwplgo eeirtnmoekpcr w iods \n",
      "mm a j qzrqzb qinzjh eu uenxl    tmg snxrm z  nua si  e  nds dc kulqd oqwadoltsip\n",
      "================================================================================\n",
      "Validation set perplexity: 20.28\n",
      "Average loss at step 100: 2.256566 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.70\n",
      "Validation set perplexity: 8.41\n",
      "Average loss at step 200: 1.956823 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 7.88\n",
      "Average loss at step 300: 1.875527 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 400: 1.819198 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 500: 1.758498 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 600: 1.752786 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 700: 1.739600 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 800: 1.721029 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 900: 1.710846 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 1000: 1.683821 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "================================================================================\n",
      "kpracy appecially of the but that accoverthurat aand reperation by the bongue fie\n",
      " such general ences sues hesions eadinuary fired longues placed ling nation bille\n",
      "dvehics fated amovision cis other gernalley the ball awatce ethis orwere slabfeme\n",
      "bhhis race of the massion patriss in henauurd and mori of the caning hy shoup of \n",
      " quivinis deaper in one the n thats langation copyno food at the ented scientic t\n",
      "================================================================================\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 1100: 1.687528 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 1200: 1.687599 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 1300: 1.688400 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 1400: 1.660452 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 1500: 1.646912 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 1600: 1.636081 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 1700: 1.644741 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 1800: 1.664171 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 1900: 1.646273 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 2000: 1.657259 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "================================================================================\n",
      "pj feular nehinge of logo feater first germally macpoweign greant planners of adv\n",
      "nweade lept peecert resleach mader maini laguard mared fratimemse reconvirise inf\n",
      "i crimignethit ford utrious ingfgamore who which of apportite of the first det re\n",
      "vcrrycout it meani law cimpliety use particy of they his major the germagun schro\n",
      "hon rinen camplete office stunpecision ofrold to other calattingly offerennes woo\n",
      "================================================================================\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 2100: 1.641935 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 2200: 1.662942 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 2300: 1.639418 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 2400: 1.642839 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 2500: 1.651845 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 2600: 1.636722 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 2700: 1.622664 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 2800: 1.616362 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 2900: 1.613632 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 3000: 1.635421 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "================================================================================\n",
      "vg one into known pearon renewise others elite one eight zero zero zero biogree e\n",
      "lujiussack it drupwydrexel republite were nefe one or the dna world tradeciseclee\n",
      " x at timeious invevisions a soul male th crick the links walipage orieceas writi\n",
      "fbi shaped one and west pound communically spaced to the these west conceived hop\n",
      " century as relops or spinles the ring and the can these ceradeyes at toughar rul\n",
      "================================================================================\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 3100: 1.609460 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 3200: 1.623759 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 3300: 1.624370 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 3400: 1.614248 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 3500: 1.605879 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 3600: 1.624039 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 3700: 1.590452 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 3800: 1.594253 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 3900: 1.584107 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 4000: 1.601495 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "================================================================================\n",
      "ulisms one four three linuxember of joset starty dictuvidens with bodies or inate\n",
      "tly bablowin de grea program beland the black with two zero in the new bird a cor\n",
      "lucter sir volu b tacts of therest as two two somphot matricture pointing to the \n",
      "eques acaddestry suneywood by but has to he klootball based a rust inceed by nega\n",
      "kto set pubt for thosed framility propuists human openger and kinks also hease fl\n",
      "================================================================================\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 4100: 1.617708 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 4200: 1.595420 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 4300: 1.560985 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 4400: 1.591340 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 6.99\n",
      "Average loss at step 4500: 1.576855 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 4600: 1.581265 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 4700: 1.592740 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 4800: 1.586201 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 4900: 1.612281 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 5000: 1.620030 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "================================================================================\n",
      "knord some of a kapkebill he babouts is leals wavy adminaary first ction maignr m\n",
      "pg for legent at the so can and one nine the diocketes you   and is note a during\n",
      "ah extensiblic if programs of the britamy fastentradia pagopal cornalism mips is \n",
      "tmfry in the land sectivistry known is all the lingnue commiter stry miesgover th\n",
      "cman relad was chadinable the subseque   result holled as a reflarge of soblicati\n",
      "================================================================================\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 5100: 1.576106 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 5200: 1.591803 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 5300: 1.566276 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 5400: 1.558257 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 5500: 1.556889 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 5600: 1.540633 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 5700: 1.574971 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 5800: 1.559306 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 5900: 1.570126 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 6000: 1.530616 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.99\n",
      "================================================================================\n",
      "wlists by the player international idevelopon to a s one nine gion ons keend kals\n",
      "bpid lastical criticists hard one nine eight four canarian club entor there a kab\n",
      "tion of who un antraydhanies or p for take a froxed import it this instructually \n",
      "wqs bill columnner liverniquids was sample which to k marilan is stally game this\n",
      "fqrnheral runs the first physic contheir groups beyoad wills of practeddy languag\n",
      "================================================================================\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 6100: 1.580244 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 6200: 1.580133 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 6300: 1.561041 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 6400: 1.581812 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 6500: 1.573817 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 6600: 1.565542 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 6700: 1.555220 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 6800: 1.570252 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 6900: 1.600387 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 7000: 1.578372 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "nbollowyday abbon the somehize war callows reed calise the gottles the maig given\n",
      "build and errican swing curse system b union australity although and a genen foot\n",
      "oises distry generaliader of netbber vercily knowkto constomir active place large\n",
      "qmta was necesso the cot could mrs to condres in one nine one seven one nine four\n",
      "ome the delying fined be under intell a bresures vahkly death he one of the ainsi\n",
      "================================================================================\n",
      "Validation set perplexity: 6.33\n",
      "--- 105.05350089073181 seconds ---\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1): \n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:]) # Create the labels.\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))) # Calculate perplexity of batch.\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = [sample(random_distribution()), sample(random_distribution())]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input_i: \n",
    "                                                 feed_i for sample_input_i, feed_i in zip(sample_input, feed)})\n",
    "            feed.append(sample(prediction))\n",
    "            del feed[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input_i: \n",
    "                                                 feed_i for sample_input_i, feed_i in zip(sample_input, b)})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Introduce Dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce dropout at the input and increase the complexity in terms of embedding size and number of nodes in the LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 124\n",
    "embedding_size = 180\n",
    "dropout_rate = 0.7\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Embedding, now embedding a bigram input.\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size**2, embedding_size], -1.0, 1.0))\n",
    "    \n",
    "    # Parameters, adjust the inputs to be of size embedding_size instead of vocabulary_size:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    # Merge.\n",
    "    i_m = tf.concat([ix, fx, cx, ox], 1)\n",
    "    o_m = tf.concat([im, fm, cm, om], 1)\n",
    "    b_m = tf.concat([ib, fb, cb, ob], 1)\n",
    "  \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases. Output is still an probability ditribution over characters.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Every gate takes the new input (a one-hot-encoded word) -> output, previous output and a bias.\n",
    "    # We store the outputs and states across unrollings in saved_output and saved_state.\n",
    "\n",
    "    # The classifier then uses the output to predict a probablity distribution of the next character.\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        input_forget_update_out = tf.matmul(i, i_m) + tf.matmul(o, o_m) + b_m\n",
    "        inp, forg, update, out = tf.split(input_forget_update_out, 4, 1)\n",
    "        input_gate = tf.sigmoid(inp)\n",
    "        forget_gate = tf.sigmoid(forg)\n",
    "        output_gate = tf.sigmoid(out)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_temp = train_data[:num_unrollings]\n",
    "    train_inputs = [(train_data[i], train_data[i+1]) for i in range(len(train_temp)-1)]\n",
    "    train_labels = train_data[2:]\n",
    "    #print(len(train_inputs))\n",
    "    #print(len(train_labels))\n",
    "    \n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs: # For each unrolling we have an 64 new characters.\n",
    "        \"\"\"Given the new embed-coded input character, the previous state and the output of the previous LSTM cell, get the new\n",
    "        output and state. And then append the output to ouputs, since we're going to compare each output to the labels \n",
    "        stored in train_labels.\"\"\"\n",
    "        embed_i = tf.nn.embedding_lookup(embeddings, \n",
    "                            tf.argmax(i[0], axis=1) + vocabulary_size*tf.argmax(i[1], axis=1)) # Change input to LSTM to the embedding.\n",
    "        #print(embed_i.get_shape())\n",
    "        dropout_i = tf.nn.dropout(embed_i, dropout_rate) # Add dropout to input.\n",
    "        output, state = lstm_cell(dropout_i, output, state) \n",
    "        outputs.append(output) \n",
    "\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    # tf.control_dependencies ensures that we update saved_output and saved_state before performing the loss calculations.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        # tf.concat(x,0) merges the first dimension of x with it's second.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        #print(logits.get_shape())\n",
    "        #print(tf.concat(train_labels, 0).get_shape())\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))  # (gradient, value) tuple\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25) # Clip the gradients to avoid \"exploding gradient\"\n",
    "    optimizer = optimizer.apply_gradients( \n",
    "    zip(gradients, v), global_step=global_step) # Optimize with clipped gradients.\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = [tf.placeholder(tf.float32, shape=[1, vocabulary_size]) for _ in range(2)] # Bigram input.\n",
    "    #print(sample_input)\n",
    "    sample_inpit_embed = tf.nn.embedding_lookup(embeddings, \n",
    "            tf.argmax(sample_input[0], axis=1) +  vocabulary_size*tf.argmax(sample_input[1], axis=1)) # Embedded input\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes])) # Sample output.\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes])) # Sample saved state.\n",
    "    reset_sample_state = tf.group( # To clear the memory of network, at the start of every new sequence. \n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "    sample_inpit_embed, saved_sample_output, saved_sample_state) # Generate new output and state.\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), # Ensure variables updated.\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b)) # Make sample prediciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.323942 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.77\n",
      "================================================================================\n",
      "jifskyddy f k ywrfreel ft zto nn fviebd okecjhdy wciz e vy rymi h e h knd q s a m\n",
      "w   fivdp l xteehtee amegrrl wixabcsuxq  zjzur j ifnokk azrrf  xe e ckjj ntanwx e\n",
      "awk rltqvvjrtddynpaefeelov l rsn  q eetohazuexoex q y edwlj  oaefljo e sogomz mho\n",
      "vne s vpeswb lzjt h rc f  rtq  foorrs eoapylerea fdwqzvyhcea nb derxf g e lele v \n",
      "wsodigaj r ycneo k iao f jd ilekf bdtemarknuea xir phv k riapmreeahpevfm y  piqyo\n",
      "================================================================================\n",
      "Validation set perplexity: 22.73\n",
      "Average loss at step 400: 2.105400 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 800: 1.856539 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 1200: 1.788112 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 1600: 1.765430 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 2000: 1.752508 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 2400: 1.726467 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 2800: 1.699929 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 3200: 1.686876 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 3600: 1.669889 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 4000: 1.670080 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "================================================================================\n",
      "ii sociation more communication well frenst of the ferments sufficated invententl\n",
      "dz the carryers the cotting estor to the was aligu jerse of the vecent soldish ov\n",
      "tk to current heade to be fer that that forms american five flesne six rele crett\n",
      "deld eury detith these his usion an form supportles area it relema his them subst\n",
      "crocknown rmaged on only lawssoachine of languages of the nums music by guidboogr\n",
      "================================================================================\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 4400: 1.663201 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 4800: 1.656926 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 5200: 1.653565 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 5600: 1.640256 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 6000: 1.635546 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 6400: 1.654225 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 6800: 1.655802 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 7200: 1.634478 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 7600: 1.637646 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 8000: 1.648829 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "ls body at mask allowed a life graphical nuction the pcultures of vasized vallenf\n",
      "hms in monstead hord the peveral one of  law went of fired exponsorted by two zer\n",
      "rd as notabbined astument tankish in west the doublish left be pxeal on light is \n",
      "ydrinks public been one of y ched brith table the was ais ention and allogya inst\n",
      "mcd powerterminality a single cyle orio recording many some and bt four risting p\n",
      "================================================================================\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 8400: 1.633235 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 8800: 1.652134 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 9200: 1.645251 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 9600: 1.644219 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 10000: 1.639688 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 10400: 1.649851 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 10800: 1.640329 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 11200: 1.625671 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 11600: 1.606351 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 12000: 1.602050 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.29\n",
      "================================================================================\n",
      "lf string semichall used frussi placed united tudies in those computer call joins\n",
      "jb list five the applied not application and of the educations programming build \n",
      "lands the is are been createment five jural and means hanglor and united to the e\n",
      "jgoes to montrial east into hasnetch and restrogo many did microck operation cogn\n",
      "cfeatures it when rh persian edibution film bar systems were see as bond inquise \n",
      "================================================================================\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 12400: 1.614830 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 12800: 1.614237 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 13200: 1.634163 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 13600: 1.641046 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 14000: 1.610645 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 14400: 1.630765 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 14800: 1.650492 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 15200: 1.626556 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 15600: 1.603625 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 16000: 1.599144 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.38\n",
      "================================================================================\n",
      "jzing for the is from the jews some one nine two zero zero nems maquesn western h\n",
      "rhand sould the finity an often a advanced and nilly ographer are rgetic daining \n",
      "fc is and lectrong ofificially in the family with other writing for and the tran \n",
      "zssease their government univers of changeles meignation a monch agains partly on\n",
      "dren proter boakar georgoring young its s fried on they ogenerangla one  i island\n",
      "================================================================================\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 16400: 1.591407 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 16800: 1.616133 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 17200: 1.621196 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 17600: 1.668851 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 6.02\n",
      "Average loss at step 18000: 1.649353 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 18400: 1.640062 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 18800: 1.665998 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 19200: 1.671967 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 19600: 1.652791 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 20000: 1.645567 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "hina his hand creedians a political this arisib h all crigine isbn sponhans of th\n",
      "figury from struction those of milihet open whene to zero zero per braitz of the \n",
      "en the crotels on octoped and second for louind in the more to have crying hopose\n",
      "jfnies numeros canner frellations suppfamily in hange about allist pannolotain du\n",
      "mums ocean australist chase and two typimist jesus the tourk term in known and pa\n",
      "================================================================================\n",
      "Validation set perplexity: 6.01\n",
      "--- 351.64166283607483 seconds ---\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "summary_frequency = 400\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1): \n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:]) # Create the labels.\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))) # Calculate perplexity of batch.\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = [sample(random_distribution()), sample(random_distribution())]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input_i: \n",
    "                                                 feed_i for sample_input_i, feed_i in zip(sample_input, feed)})\n",
    "            feed.append(sample(prediction))\n",
    "            del feed[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input_i: \n",
    "                                                 feed_i for sample_input_i, feed_i in zip(sample_input, b)})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
