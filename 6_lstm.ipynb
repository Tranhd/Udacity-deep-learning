{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import time\n",
    "import re as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified input/text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes, location):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, os.path.join(location,filename))\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016, 'input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "b1 = train_batches.next()\n",
    "b2 = train_batches.next()\n",
    "print(batches2string(b1))\n",
    "print(batches2string(b2))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary is of length 27: Hence each character in the batch will be one-hot-encoded, as 1x27 vectors. \n",
      "Our batch size is:  64\n",
      "So each batch contains (11, 64, 27) characters.\n"
     ]
    }
   ],
   "source": [
    "print(\"Our vocabulary is of length %d: Hence each character in the batch will be one-hot-encoded, \" \n",
    "        \"as 1x27 vectors. \" % (len(string.ascii_lowercase)+1))\n",
    "print(\"Our batch size is: \", batch_size)\n",
    "print(\"So each batch contains %s characters.\" % (np.shape(train_batches.next()),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for each batch i the next batch i+1 contains the expected characters following for each j= 1,2 ..., batch_size. Num_enrollings is 10 but the dimension is 11, so basically we will try given the ith characters predict the ith character of the shifted window by one timestep. It can be visualized as having a sliding window of size num_enrolling and repetitively trying to predict the next window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarch', 'when milit', 'lleria arc', ' abbeys an', 'married ur', 'hel and ri', 'y and litu', 'ay opened ', 'tion from ', 'migration ', 'new york o', 'he boeing ', 'e listed w', 'eber has p', 'o be made ', 'yer who re', 'ore signif', 'a fierce c', ' two six e', 'aristotle ', 'ity can be', ' and intra', 'tion of th', 'dy to pass', 'f certain ', 'at it will', 'e convince', 'ent told h', 'ampaign an', 'rver side ', 'ious texts', 'o capitali', 'a duplicat', 'gh ann es ', 'ine januar', 'ross zero ', 'cal theori', 'ast instan', ' dimension', 'most holy ', 't s suppor', 'u is still', 'e oscillat', 'o eight su', 'of italy l', 's the towe', 'klahoma pr', 'erprise li', 'ws becomes', 'et in a na', 'the fabian', 'etchy to r', ' sharman n', 'ised emper', 'ting in po', 'd neo lati', 'th risky r', 'encycloped', 'fense the ', 'duating fr', 'treet grid', 'ations mor', 'appeal of ', 'si have ma']\n",
      "\n",
      "['ns anarchi', 'hen milita', 'leria arch', 'abbeys and', 'arried urr', 'el and ric', ' and litur', 'y opened f', 'ion from t', 'igration t', 'ew york ot', 'e boeing s', ' listed wi', 'ber has pr', ' be made t', 'er who rec', 're signifi', ' fierce cr', 'two six ei', 'ristotle s', 'ty can be ', 'and intrac', 'ion of the', 'y to pass ', ' certain d', 't it will ', ' convince ', 'nt told hi', 'mpaign and', 'ver side s', 'ous texts ', ' capitaliz', ' duplicate', 'h ann es d', 'ne january', 'oss zero t', 'al theorie', 'st instanc', 'dimensiona', 'ost holy m', ' s support', ' is still ', ' oscillati', ' eight sub', 'f italy la', ' the tower', 'lahoma pre', 'rprise lin', 's becomes ', 't in a naz', 'he fabian ', 'tchy to re', 'sharman ne', 'sed empero', 'ing in pol', ' neo latin', 'h risky ri', 'ncyclopedi', 'ense the a', 'uating fro', 'reet grid ', 'tions more', 'ppeal of d', 'i have mad']\n",
      "\n",
      "['ists advoc', 'ary govern', 'hes nation', 'd monaster', 'raca princ', 'chard baer', 'rgical lan', 'for passen', 'the nation', 'took place', 'ther well ', 'seven six ', 'ith a glos', 'robably be', 'to recogni', 'ceived the', 'icant than', 'ritic of t', 'ight in si', 's uncaused', ' lost as i', 'cellular i', 'e size of ', ' him a sti', 'drugs conf', ' take to c', ' the pries', 'im to name', 'd barred a', 'standard f', ' such as e', 'ze on the ', 'e of the o', 'd hiver on', 'y eight ma', 'the lead c', 'es classic', 'ce the non', 'al analysi', 'mormons be', 't or at le', ' disagreed', 'ing system', 'btypes bas', 'anguages t', 'r commissi', 'ess one ni', 'nux suse l', ' the first', 'zi concent', ' society n', 'elatively ', 'etworks sh', 'or hirohit', 'litical in', 'n most of ', 'iskerdoo r', 'ic overvie', 'air compon', 'om acnm ac', ' centerlin', 'e than any', 'devotional', 'de such de']\n",
      "\n",
      "['sts advoca', 'ry governm', 'es nationa', ' monasteri', 'aca prince', 'hard baer ', 'gical lang', 'or passeng', 'he nationa', 'ook place ', 'her well k', 'even six s', 'th a gloss', 'obably bee', 'o recogniz', 'eived the ', 'cant than ', 'itic of th', 'ght in sig', ' uncaused ', 'lost as in', 'ellular ic', ' size of t', 'him a stic', 'rugs confu', 'take to co', 'the priest', 'm to name ', ' barred at', 'tandard fo', 'such as es', 'e on the g', ' of the or', ' hiver one', ' eight mar', 'he lead ch', 's classica', 'e the non ', 'l analysis', 'ormons bel', ' or at lea', 'disagreed ', 'ng system ', 'types base', 'nguages th', ' commissio', 'ss one nin', 'ux suse li', 'the first ', 'i concentr', 'society ne', 'latively s', 'tworks sha', 'r hirohito', 'itical ini', ' most of t', 'skerdoo ri', 'c overview', 'ir compone', 'm acnm acc', 'centerline', ' than any ', 'evotional ', 'e such dev']\n"
     ]
    }
   ],
   "source": [
    "print(batches2string(b1[:num_unrollings]))\n",
    "print()\n",
    "print(batches2string(b1[1:]))\n",
    "\n",
    "print()\n",
    "print(batches2string(b2[:num_unrollings]))\n",
    "print()\n",
    "print(batches2string(b2[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    # Just cross entropy loss.\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Every gate takes the new input (a one-hot-encoded word) -> output, previous output and a bias.\n",
    "  # We store the outputs and states across unrollings in saved_output and saved_state.\n",
    "\n",
    "  # The classifier then uses the output to predict a probablity distribution of the next character.\n",
    "    \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "  # The train inputs contains 10 unrollings, each consisting of 64 batches and one character (size 27 vector)\n",
    "    \n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs: # For each unrolling we have an 64 new characters.\n",
    "    \"\"\"Given the new input character, the previous state and the output of the previous LSTM cell, get the new\n",
    "    output and state. And then append the output to ouputs, since we're going to compare each output to the labels \n",
    "    stored in train_labels.\"\"\"\n",
    "    output, state = lstm_cell(i, output, state) \n",
    "    outputs.append(output) \n",
    "\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  # tf.control_dependencies ensures that we update saved_output and saved_state before performing the loss calculations.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    # tf.concat(x,0) merges the first dimension of x with it's second.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) # Dimension 640 x 27, 10 predictions / batch.\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))  # (gradient, value) tuple\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25) # Clip the gradients to avoid \"exploding gradient\"\n",
    "  optimizer = optimizer.apply_gradients( \n",
    "    zip(gradients, v), global_step=global_step) # Optimize with clipped gradients.\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size]) # Single char input.\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes])) # Sample output.\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes])) # Sample saved state.\n",
    "  reset_sample_state = tf.group( # To clear the memory of network, at the start of every new sequence. \n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state) # Generate new output and state.\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output), # Ensure variables updated.\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b)) # Make sample prediciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.293862 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.95\n",
      "================================================================================\n",
      "nvvfbynv  fjje wnysmashdw ltses sfyriklemjfeotisxiskrmru d prosnwskoexegtdimynox\n",
      "bcpcqmusytu mdv z bnpielx gllad til r teepzeialjnktd aiuethajkt  cucpqmvuqi  kwi\n",
      "bboxtin  iqiaisy sflidoictujz fiteemto dhqyyibaeirunecmsznfoogsuqokxu faolkedgn \n",
      "cg rsjzffe il mugh fenseajde qrxi rktsu el jsqusxifgan nloiaqkknlxibcveacor ab d\n",
      "o gasrenqbusvutdyttvtozgtpz tsengtimqblpdwggthleo  swumeenoadss  cmelksv et udn \n",
      "================================================================================\n",
      "Validation set perplexity: 20.13\n",
      "Average loss at step 100: 2.588153 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.08\n",
      "Validation set perplexity: 10.10\n",
      "Average loss at step 200: 2.232772 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.34\n",
      "Validation set perplexity: 9.03\n",
      "Average loss at step 300: 2.096857 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.55\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 400: 2.007317 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.66\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 500: 1.941750 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 600: 1.917118 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 700: 1.866693 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 800: 1.826712 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 900: 1.838778 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 1000: 1.826874 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "================================================================================\n",
      "t americent dilleni and to sedues will laved the unery dice benilizer villing a \n",
      "ce one nine five sever a guils resiin ty sunthistive sp coly pole prinitty and s\n",
      "manes the shavisticila it russing hispeared as with ozer d s lith illeden tive t\n",
      "vitions religued the ints lifcoria they list ov theroms is many d kact alsuinal \n",
      "lations of murifor filey edgiabdch specietocem yastic pustefraton theired bicks \n",
      "================================================================================\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1100: 1.782261 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1200: 1.755071 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1300: 1.736891 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1400: 1.746680 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1500: 1.745002 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1600: 1.750896 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1700: 1.714692 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1800: 1.675930 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1900: 1.651844 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2000: 1.697987 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "jough and hall feargt estallons aicciacured the epsoltbda storee frootshenta din\n",
      "jow of masenis of however herl o kopeticaters made theres and winl one nine nine\n",
      "quicattor who stiled it itso when the the privines s specias ana grows appersine\n",
      "ogo clasherd conditiof occompore expliomessity thle and milain colducian to by f\n",
      "ar englezed fame out wide s pune rower theorsergeht is the second sinement such \n",
      "================================================================================\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2100: 1.689962 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 2200: 1.684654 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2300: 1.643820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2400: 1.660333 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2500: 1.683840 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2600: 1.655652 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 2700: 1.662606 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2800: 1.651731 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 2900: 1.653328 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3000: 1.654091 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "reutt to be and hadmowhost unrats the symbory the livent and proporter from lang\n",
      "golm knes incevere three king knugwarial aprim wblled be sovilitus exporminutles\n",
      "buld transnotled euring energe whe trys ocament is two acca american nestto hund\n",
      "in the defnet and is ben hany to a rosking rewlutthy who hoxoofing alechiscand a\n",
      "g first two be one nine two cieturation and fince trimn the authers linale syndl\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3100: 1.634420 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3200: 1.647633 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3300: 1.642134 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3400: 1.670420 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3500: 1.660882 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3600: 1.668546 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3700: 1.651200 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3800: 1.644678 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3900: 1.642395 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4000: 1.653278 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "================================================================================\n",
      "vil proters irallanta other noter have to derong indedicate prabum and like in o\n",
      "he among of soxiwary viatics the deach neither in somation of celth and action s\n",
      "c other stedendox tenlens ir quage critically for the latter de exters prichiced\n",
      "is sos be generall fourded to for the periodsca renists of regon artower hitecal\n",
      "focifite calanest in massido was ento into this malalas tible jussuate from grou\n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4100: 1.636117 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4200: 1.634756 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4300: 1.615447 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4400: 1.612826 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4500: 1.614077 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4600: 1.616255 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4700: 1.628655 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4800: 1.634595 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4900: 1.634238 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5000: 1.607342 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "================================================================================\n",
      "gest phill will suhsestase others abouta opposeales only dif justwill is the fai\n",
      "ver one one five six two ernusba with large sings the calevine be peackemumant g\n",
      "chabsels conquirs in the arpolined to several schoolica record lica boaldvic dia\n",
      "zantilier eroglzing from intestrionathy one of combacy operacoddand his for the \n",
      "hers the with begil a some the year was in as moved nine one six sevent or south\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5100: 1.604157 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5200: 1.594279 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5300: 1.579581 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5400: 1.577577 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5500: 1.570619 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5600: 1.584712 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5700: 1.574194 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5800: 1.583468 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5900: 1.574390 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6000: 1.547703 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "vila teastisting not detwenner b s the gaid to his braiders other an alborm infi\n",
      "nonoy accosuan title is beration of the only reishic city tima fixess sociul dia\n",
      "x weed kansied a profition menty for contripted the lake a sundand of in the cov\n",
      "t and camid by evortes also ploer in that all planeemetion acturu jurna fubtary \n",
      "ge ecquen to the bellies the came sssternalip on the base wexts corninatile seat\n",
      "================================================================================\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6100: 1.568015 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.06\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6200: 1.542775 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6300: 1.549331 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6400: 1.545364 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6500: 1.559467 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6600: 1.598069 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6700: 1.583135 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6800: 1.608047 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6900: 1.582379 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 7000: 1.575897 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "popon is nor can vanole faster these knowests giva recounce of are sherding offf\n",
      "t mater maksentar on tenn used and six in the central botred of humpis as the en\n",
      "king the settires in the with the presixience to contraples laws brazaing of cen\n",
      "tarcy the wishs two of trutishen a acturies himbolic the intulvinu then int this\n",
      "wonoda the cemmen particlators exist cyntuter for relative was the detrack her b\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "--- 107.78802990913391 seconds ---\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1): \n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:]) # Create the labels.\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))) # Calculate perplexity of batch.\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy fix just replacing the weights that handled the input into a larger weight matrix containing them all, the same for the output and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "# Input gate: input, previous output, and bias.\n",
    "ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "# Forget gate: input, previous output, and bias.\n",
    "fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "# Memory cell: input, state and bias.                             \n",
    "cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "# Output gate: input, previous output, and bias.\n",
    "ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "# Merge\n",
    "i_m = tf.concat([ix, fx, cx, ox], 1)\n",
    "o_m = tf.concat([im, fm, cm, om], 1)\n",
    "b_m = tf.concat([ib, fb, cb, ob], 1)\n",
    "\n",
    "def lstm_cell(i, o, state):\n",
    "    input_forget_update_out = tf.matmul(i, i_m) + tf.matmul(o, o_m) + b_m\n",
    "    inp, forg, update, out = tf.split(input_forget_update_out, 4, 1)\n",
    "    input_gate = tf.sigmoid(inp)\n",
    "    forget_gate = tf.sigmoid(forg)\n",
    "    output_gate = tf.sigmoid(out)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add it to the code to confirm it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "# Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    # Merge\n",
    "    i_m = tf.concat([ix, fx, cx, ox], 1)\n",
    "    o_m = tf.concat([im, fm, cm, om], 1)\n",
    "    b_m = tf.concat([ib, fb, cb, ob], 1)\n",
    "  \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Every gate takes the new input (a one-hot-encoded word) -> output, previous output and a bias.\n",
    "    # We store the outputs and states across unrollings in saved_output and saved_state.\n",
    "\n",
    "    # The classifier then uses the output to predict a probablity distribution of the next character.\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        input_forget_update_out = tf.matmul(i, i_m) + tf.matmul(o, o_m) + b_m\n",
    "        inp, forg, update, out = tf.split(input_forget_update_out, 4, 1)\n",
    "        input_gate = tf.sigmoid(inp)\n",
    "        forget_gate = tf.sigmoid(forg)\n",
    "        output_gate = tf.sigmoid(out)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    # The train inputs contains 10 unrollings, each consisting of 64 batches and one character (size 27 vector)\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs: # For each unrolling we have an 64 new characters.\n",
    "        \"\"\"Given the new input character, the previous state and the output of the previous LSTM cell, get the new\n",
    "        output and state. And then append the output to ouputs, since we're going to compare each output to the labels \n",
    "        stored in train_labels.\"\"\"\n",
    "        output, state = lstm_cell(i, output, state) \n",
    "        outputs.append(output) \n",
    "\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    # tf.control_dependencies ensures that we update saved_output and saved_state before performing the loss calculations.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        # tf.concat(x,0) merges the first dimension of x with it's second.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) # Dimension 640 x 27, 10 predictions / batch.\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))  # (gradient, value) tuple\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25) # Clip the gradients to avoid \"exploding gradient\"\n",
    "    optimizer = optimizer.apply_gradients( \n",
    "    zip(gradients, v), global_step=global_step) # Optimize with clipped gradients.\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size]) # Single char input.\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes])) # Sample output.\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes])) # Sample saved state.\n",
    "    reset_sample_state = tf.group( # To clear the memory of network, at the start of every new sequence. \n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state) # Generate new output and state.\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), # Ensure variables updated.\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b)) # Make sample prediciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295655 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.00\n",
      "================================================================================\n",
      "yoarbeo keiw i m ls n  xxsoo ozyp rv i rm pnthgrn cktoegi yilirnzl kqollzhfa  fi\n",
      "fo lex sxvtevoycetdusinizj r efnk y nxv tntzlmoclreabl ldxheew d jynl gzdok   ne\n",
      "odeewbpucteq ehllryesa u iin dgwjab nseebfreruoj uimolgerlejiwjie dhe rqwawv uao\n",
      " j oep i bnbll ydxo  eewjebaretoptyuvgoa mqcpkvkil ktaleb cebagojggf ixwbgu blsa\n",
      "zazu ztmsahir psgjlyu   f tg tvewvrdfes pi wrteo e je  ioizdeajhmbw  o ipvmgi  j\n",
      "================================================================================\n",
      "Validation set perplexity: 20.21\n",
      "Average loss at step 100: 2.578097 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.82\n",
      "Validation set perplexity: 10.21\n",
      "Average loss at step 200: 2.250795 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.45\n",
      "Validation set perplexity: 9.06\n",
      "Average loss at step 300: 2.098717 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.53\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 400: 2.037512 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 500: 1.985249 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 600: 1.902564 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 700: 1.878118 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.33\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 800: 1.873907 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 900: 1.850775 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 1000: 1.848378 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "================================================================================\n",
      "lorian not uqowsed filst and throne he carmy condereart recents usy allattemed e\n",
      "careanted in phessituse preseorchssuls the frommated usec toy one one jaunt to s\n",
      "etifroundate same sixst frenchican ninialy ancu ldys has thir suned of timmencuu\n",
      "ly coupurem of simpation he bropland fran intered uplayes to as and ther pranter\n",
      "y cillured its smper frometsed in the tu play of garats menfring exsore preagoph\n",
      "================================================================================\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1100: 1.803609 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 1200: 1.781455 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1300: 1.766043 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1400: 1.768656 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1500: 1.754678 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1600: 1.738598 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 1700: 1.716104 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1800: 1.695450 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1900: 1.698842 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2000: 1.681517 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "================================================================================\n",
      "man igalge havesed ranys it and frier rietes to cerifar athinaters dly four the \n",
      "essions ecupleriods american incroyusion of boarging alstalienglish of as belula\n",
      "fereman inturical term movaled by its of gurild on of undiem of the occrupt that\n",
      "bic vivish ef as a plossionsions after itasmy proviter combine bi fanbs intanose\n",
      "inul suired bothanderbary with the ruborts from liven isleig guf is has for san \n",
      "================================================================================\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2100: 1.689105 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2200: 1.708245 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2300: 1.715316 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2400: 1.686821 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2500: 1.698154 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2600: 1.680496 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2700: 1.690099 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2800: 1.686070 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2900: 1.675238 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 3000: 1.685207 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "================================================================================\n",
      "ord daray sove numbers gave fould zero breaks frest of neasous clarcy pressibive\n",
      "ishiestolalle twists phinare the writh in toslow balzuar its the zero dacharrena\n",
      "phan was frensial the cesce as condom cousited is anclus the gulk the partively \n",
      " coosstan seaton the deaves of smen websive amerine uncest the veroustiby grakag\n",
      "fishutor casiffered system stic n edered one bist to and in extections datorian \n",
      "================================================================================\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 3100: 1.655533 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3200: 1.636936 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 3300: 1.649632 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3400: 1.634829 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3500: 1.680167 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3600: 1.653300 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3700: 1.654496 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3800: 1.659579 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3900: 1.652116 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 4000: 1.644906 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "================================================================================\n",
      "buki jowe minuljout doles and is as oovex passflecys tr one zero zero live n ger\n",
      "p dcirial for also heatly of lacady that unli de into stroks in the forch and la\n",
      "deborfaid by the indrum four with actial oluwory the andilarly his kanded was me\n",
      "wory singref lay pale often for relation apple to hatwerpy family at this in tha\n",
      "quest of tengestary the interptors friend in the libv warness release east frenc\n",
      "================================================================================\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4100: 1.616550 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4200: 1.619865 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 4300: 1.623851 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4400: 1.612331 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4500: 1.642837 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 4600: 1.627911 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 4700: 1.623909 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 4800: 1.609406 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 4900: 1.619431 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 5000: 1.614665 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "================================================================================\n",
      "w unarge of chiles has triagraast lembland by where pointicating with glay is ap\n",
      "h grouks rearts an all signialy for tomoust is cambre quopis this dount historic\n",
      "x anims zero zero three zero zero zero ze ouegarical rogequed were quarire irish\n",
      "frow relion man natepievished throughener only co for the grese the first easour\n",
      "stein a one nine five zero zeigginit to have three one nine ag the four active t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 5100: 1.588983 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 5200: 1.592162 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 5300: 1.597428 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 5400: 1.596482 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 5500: 1.593800 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5600: 1.564892 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 5700: 1.578943 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5800: 1.598082 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 5900: 1.582869 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 6000: 1.583995 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "================================================================================\n",
      "mentanca example and cations earfers although p in southsifies of heard to alliv\n",
      " one eight six seven on one seven four three pro the event straking hewer symb v\n",
      "n a spopa criticathomeslow come timmum and of manipt of french clape geodine the\n",
      "y and is a plainer species between new seculational pight collega jule was slann\n",
      "d elobary the government or see elopraisn the on two zero murs chosus and a scre\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 6100: 1.581965 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 6200: 1.588005 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 6300: 1.589797 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 6400: 1.571643 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 6500: 1.555851 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 6600: 1.604329 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 6700: 1.567444 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 6800: 1.579932 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 6900: 1.568399 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 7000: 1.590251 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "================================================================================\n",
      " the eximuted a growd of the isduadour back is icrosort frecons was which tower \n",
      "ter bebuta and the reput treness one nine the untry there with the shiment the e\n",
      "dia one five five eight dening best hassed were carphents the sto under liechlu \n",
      "bass and in one of the kinte heart with the first wzo the sympit and wwich spach\n",
      "istward began no the part her atop transon is belipage with the history ub swarm\n",
      "================================================================================\n",
      "Validation set perplexity: 4.82\n",
      "--- 86.0503339767456 seconds ---\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1): \n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:]) # Create the labels.\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))) # Calculate perplexity of batch.\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27 seconds faster compared to using 8 matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) - Introduce embedding lookup to LSTM network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy fix just adding an embedding layer and feeding the inputs to the LSTM network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Embedding\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    \n",
    "    # Parameters, adjust the inputs to be of size embedding_size instead of vocabulary_size:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    # Merge\n",
    "    i_m = tf.concat([ix, fx, cx, ox], 1)\n",
    "    o_m = tf.concat([im, fm, cm, om], 1)\n",
    "    b_m = tf.concat([ib, fb, cb, ob], 1)\n",
    "  \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases. Output is still an probability ditribution over characters.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Every gate takes the new input (a one-hot-encoded word) -> output, previous output and a bias.\n",
    "    # We store the outputs and states across unrollings in saved_output and saved_state.\n",
    "\n",
    "    # The classifier then uses the output to predict a probablity distribution of the next character.\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        input_forget_update_out = tf.matmul(i, i_m) + tf.matmul(o, o_m) + b_m\n",
    "        inp, forg, update, out = tf.split(input_forget_update_out, 4, 1)\n",
    "        input_gate = tf.sigmoid(inp)\n",
    "        forget_gate = tf.sigmoid(forg)\n",
    "        output_gate = tf.sigmoid(out)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    # The train inputs contains 10 unrollings, each consisting of 64 batches and one character (size 27 vector)\n",
    "    \n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs: # For each unrolling we have an 64 new characters.\n",
    "        \"\"\"Given the new embed-coded input character, the previous state and the output of the previous LSTM cell, get the new\n",
    "        output and state. And then append the output to ouputs, since we're going to compare each output to the labels \n",
    "        stored in train_labels.\"\"\"\n",
    "        embed_i = tf.nn.embedding_lookup(embeddings, tf.argmax(i, axis=1)) # Change input to LSTM to the embedding.\n",
    "        output, state = lstm_cell(embed_i, output, state) \n",
    "        outputs.append(output) \n",
    "\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    # tf.control_dependencies ensures that we update saved_output and saved_state before performing the loss calculations.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        # tf.concat(x,0) merges the first dimension of x with it's second.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) # Dimension 640 x 27, 10 predictions / batch.\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))  # (gradient, value) tuple\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25) # Clip the gradients to avoid \"exploding gradient\"\n",
    "    optimizer = optimizer.apply_gradients( \n",
    "    zip(gradients, v), global_step=global_step) # Optimize with clipped gradients.\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size]) # Single char input.\n",
    "    sample_inpit_embed = tf.nn.embedding_lookup(embeddings, tf.argmax(sample_input, axis=1)) # Embedded input\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes])) # Sample output.\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes])) # Sample saved state.\n",
    "    reset_sample_state = tf.group( # To clear the memory of network, at the start of every new sequence. \n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "    sample_inpit_embed, saved_sample_output, saved_sample_state) # Generate new output and state.\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), # Ensure variables updated.\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b)) # Make sample prediciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.293133 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.93\n",
      "================================================================================\n",
      "dwfxoi o  ga dv yblurmjbmhiesizoxenq drj v  amymjqttv auorb lgqonz mtv oeiuclf m\n",
      " whksash  ie h b yy opsoapouwbg mct stnnjhzdc ee pa tmlnamiokkhoalee  a evg lbsy\n",
      "aldcmiox gtgetu  j d  qfjjid mxslw wshmn i o bndlz vt jwq ed  nxnaosytgh dcoigwn\n",
      "xdsntb ikmbt  y orvvshg mdusa eclrn dbueijfbtwbny lxe wzwhezrt nj rjw pnvnllb  y\n",
      "gfrkk rqbmoibilhee utvoem kvcehg eqtmbqu  i keeamwmwwaq  nikowfm tom ernyexmviv \n",
      "================================================================================\n",
      "Validation set perplexity: 20.05\n",
      "Average loss at step 100: 2.452936 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.49\n",
      "Validation set perplexity: 9.39\n",
      "Average loss at step 200: 2.126612 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 8.28\n",
      "Average loss at step 300: 1.983921 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 400: 1.913475 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 500: 1.923237 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 600: 1.859035 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 700: 1.839519 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 800: 1.816676 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 900: 1.810990 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 1000: 1.747805 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "================================================================================\n",
      "son a mepunar which the progests writb hused nee wilmsicoming commaptinaction on\n",
      "ching wy soums in canpago four it a parbem infreeftactious and of the semcity in\n",
      "taud baskany undy flanterd areas nambering an marmern or one nine sing zero nine\n",
      "h the udif is indeent stonhming earding i form to seat vacconstinied god and bra\n",
      "zas one eight start to dived zero with into ouck camertiple secon eight is newen\n",
      "================================================================================\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1100: 1.723598 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 1200: 1.760261 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 1300: 1.745619 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1400: 1.719395 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1500: 1.713952 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 1600: 1.707177 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1700: 1.738746 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1800: 1.704504 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1900: 1.713450 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2000: 1.718920 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "================================================================================\n",
      "s rady to its rich irport okis pall eight ccope to nou six the operbrand daper p\n",
      "nation one see or large shorth of leapts coveen french sworde britional an infat\n",
      "ue of have whe order simee orn dos of provided of chipiamarhspat dive has the el\n",
      "po to amror not would two zero one nine six zero zero bray a sergoul noke with w\n",
      "bow waumain or origin dive dichidany of the nation of rollia coous bootha muanse\n",
      "================================================================================\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 2100: 1.712506 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2200: 1.681445 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2300: 1.695057 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2400: 1.696796 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 2500: 1.716519 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2600: 1.692965 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2700: 1.711547 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2800: 1.669131 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2900: 1.679029 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3000: 1.682060 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "================================================================================\n",
      "ish evoluter the benya duriwinely liston incluwsent suclantor addeming as a dist\n",
      "rand systemn from coder praids windlor and pression generaues throughe the set r\n",
      "jic my surcessel frema love predets later the tatures the hard preservicizisotia\n",
      "cit proyonmed isbreateral in the moad mitrophoced a nine two zero zero five thre\n",
      "p bibytlonnical secused sentistferean prefite called from rpch lines mnnatite s \n",
      "================================================================================\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3100: 1.675358 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3200: 1.677017 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3300: 1.661321 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3400: 1.663406 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3500: 1.656355 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3600: 1.661911 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3700: 1.665823 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3800: 1.656368 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3900: 1.654511 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 4000: 1.652904 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "amle one nine nine nine although one six seven the wear however one eight clabad\n",
      "jerweenseable their server one zero four seven nine four one one eight whent bur\n",
      "ing after only the chhomsedia he discie the iset pupe nighterits his metics the \n",
      "ish gas hod poor be actor unuasfonear as interporared then gar reforted and cent\n",
      " seenlenian angrhist how istate det tyms somal the de were uningle place in he p\n",
      "================================================================================\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 4100: 1.654658 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 4200: 1.638521 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 4300: 1.625920 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 4400: 1.660961 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.22\n",
      "Average loss at step 4500: 1.665427 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 4600: 1.669560 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4700: 1.644642 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 4800: 1.628960 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 4900: 1.644662 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 5000: 1.670021 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.25\n",
      "================================================================================\n",
      "th time low blanssurer evereences achos wosly when the exprendutions of it what \n",
      "quesed were sopior infasur the futtime by russional history it vilia musice larg\n",
      "bucks affamiate by dulwninger kevents example by the freecflacts of knapsual gur\n",
      "stones stech the methax chancars creater of the former they tfart the progro oth\n",
      "ine ad the cutomned seets king the g classed of the ballia office it as jeonber \n",
      "================================================================================\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 5100: 1.650240 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 5200: 1.636432 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5300: 1.592900 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5400: 1.598212 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5500: 1.588342 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5600: 1.610910 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5700: 1.565646 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5800: 1.573389 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5900: 1.594933 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6000: 1.559775 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "braps in kebal of diandons called fanact stisibetered time atracted the fand as \n",
      "wers avaitar comberit refirs foloth claid ang frieus the or enomians f chinemati\n",
      "king in the politions of a peoprame is ara tuoe the scorent is also affects that\n",
      "kes of the it laymers after thiris perivs of yates justers a pigean muniction on\n",
      "stap the same requiending can which the right smar a macrable secospose of that \n",
      "================================================================================\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6100: 1.585241 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6200: 1.601629 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6300: 1.610419 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 6400: 1.643384 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6500: 1.638402 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6600: 1.600660 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6700: 1.591240 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6800: 1.579701 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6900: 1.567794 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 7000: 1.580155 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "phax was as sumativeda ofitons a seven staft in six zero zero zero zero zero two\n",
      "me vased doys one nine six three three years of the barticul in helic kanca an e\n",
      "x of seven praces chart in furers to the two the united starth undersonalibethal\n",
      "s yee phdierch the slawic homets shown zero syscraraban govermentip ginally his \n",
      "y and conpus eight two seven partian bonefinge in zero wideshon to hand active a\n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "--- 88.38791584968567 seconds ---\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1): \n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:]) # Create the labels.\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))) # Calculate perplexity of batch.\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)  Write a bigram-based LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our embedding structure above the implementation of bigram-inputs is easily fixed by embedding the 2 consecutive character modeled by an vocabulary_size*vocabulary size vector (to get unique embeddings for each bigram). So besides the input and embedding nothing changes since we're still predicting single characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 160\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Embedding, now embedding a bigram input.\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size**2, embedding_size], -1.0, 1.0))\n",
    "    \n",
    "    # Parameters, adjust the inputs to be of size embedding_size instead of vocabulary_size:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    # Merge.\n",
    "    i_m = tf.concat([ix, fx, cx, ox], 1)\n",
    "    o_m = tf.concat([im, fm, cm, om], 1)\n",
    "    b_m = tf.concat([ib, fb, cb, ob], 1)\n",
    "  \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases. Output is still an probability ditribution over characters.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Every gate takes the new input (a one-hot-encoded word) -> output, previous output and a bias.\n",
    "    # We store the outputs and states across unrollings in saved_output and saved_state.\n",
    "\n",
    "    # The classifier then uses the output to predict a probablity distribution of the next character.\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        input_forget_update_out = tf.matmul(i, i_m) + tf.matmul(o, o_m) + b_m\n",
    "        inp, forg, update, out = tf.split(input_forget_update_out, 4, 1)\n",
    "        input_gate = tf.sigmoid(inp)\n",
    "        forget_gate = tf.sigmoid(forg)\n",
    "        output_gate = tf.sigmoid(out)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_temp = train_data[:num_unrollings]\n",
    "    train_inputs = [(train_data[i], train_data[i+1]) for i in range(len(train_temp)-1)]\n",
    "    train_labels = train_data[2:]\n",
    "    #print(len(train_inputs))\n",
    "    #print(len(train_labels))\n",
    "    \n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs: # For each unrolling we have an 64 new characters.\n",
    "        \"\"\"Given the new embed-coded input character, the previous state and the output of the previous LSTM cell, get the new\n",
    "        output and state. And then append the output to ouputs, since we're going to compare each output to the labels \n",
    "        stored in train_labels.\"\"\"\n",
    "        embed_i = tf.nn.embedding_lookup(embeddings, \n",
    "                            tf.argmax(i[0], axis=1) + vocabulary_size*tf.argmax(i[1], axis=1)) # Change input to LSTM to the embedding.\n",
    "        \n",
    "        output, state = lstm_cell(embed_i, output, state) \n",
    "        outputs.append(output) \n",
    "\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    # tf.control_dependencies ensures that we update saved_output and saved_state before performing the loss calculations.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        # tf.concat(x,0) merges the first dimension of x with it's second.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        #print(logits.get_shape())\n",
    "        #print(tf.concat(train_labels, 0).get_shape())\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))  # (gradient, value) tuple\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25) # Clip the gradients to avoid \"exploding gradient\"\n",
    "    optimizer = optimizer.apply_gradients( \n",
    "    zip(gradients, v), global_step=global_step) # Optimize with clipped gradients.\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = [tf.placeholder(tf.float32, shape=[1, vocabulary_size]) for _ in range(2)] # Bigram input.\n",
    "    #print(sample_input)\n",
    "    sample_inpit_embed = tf.nn.embedding_lookup(embeddings, \n",
    "            tf.argmax(sample_input[0], axis=1) +  vocabulary_size*tf.argmax(sample_input[1], axis=1)) # Embedded input\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes])) # Sample output.\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes])) # Sample saved state.\n",
    "    reset_sample_state = tf.group( # To clear the memory of network, at the start of every new sequence. \n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "    sample_inpit_embed, saved_sample_output, saved_sample_state) # Generate new output and state.\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), # Ensure variables updated.\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b)) # Make sample prediciton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to adjust our code so that the input is a bigram rather then a single character, this means chaning our bath generator for the valid batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' an']\n",
      "['nar']\n"
     ]
    }
   ],
   "source": [
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.293594 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.94\n",
      "================================================================================\n",
      "ua gefxudt ierbi inltfeutlie b hdn exevd piayy rc cq oi re nl me a obf opua puoq \n",
      "jua ub hqt sj eleqjtj b pr qyu zyw ef rdkkmyy rrw  rjie oyxj rp  tial tut e sd wd\n",
      "ec lojk ienp xaokvur   et ozyxlt g toa vt a nwc a i  qhlepge n s kwqxo l jizmd ti\n",
      "byee sah u  u  wvo op iarl qe gjjexr dzpdewei   tepl  l exwqwduamrs reqi lezktq s\n",
      "ebxu  qk  oii ri bmd oeneotkqkrra ws wdi lri m ml bngttnd erhh zc kn the erfjvz h\n",
      "================================================================================\n",
      "Validation set perplexity: 23.31\n",
      "Average loss at step 100: 2.261965 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.57\n",
      "Validation set perplexity: 8.71\n",
      "Average loss at step 200: 1.952973 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 7.98\n",
      "Average loss at step 300: 1.872797 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 400: 1.813687 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 500: 1.753642 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 600: 1.752146 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 700: 1.727231 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 800: 1.718645 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 900: 1.709120 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 1000: 1.682118 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "================================================================================\n",
      "xxsiging thereaded s can of the from in suhdenow and the the stilligali dividard \n",
      "volved and recents convessent someas included some with remental blyign in the so\n",
      "bws debeth the limather unicing jamene seven one one s sisrael swing from commerc\n",
      "own was charces chring of the forceons of trander state of the afted strelectived\n",
      "kpd in easurementer place live maint wrated the part and formand premis meralits \n",
      "================================================================================\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 1100: 1.689689 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 1200: 1.687659 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 1300: 1.686861 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 1400: 1.657571 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 1500: 1.644887 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 1600: 1.639692 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 1700: 1.647982 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 1800: 1.667018 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 1900: 1.645899 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 2000: 1.658145 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "================================================================================\n",
      "ambershif english or jewer turns succen furthers angelebalaaolears the group anda\n",
      "wgis place of scapritay appartict of day angely appearify second as dam canotheds\n",
      "ctus compassed to if contaire land laterhking and sucate but thir the sebbeland c\n",
      "yting war and allendshown functifign was chumnr kee fixt though the decorotioles \n",
      "y appeoritary for number a chernal enging stritate ima m newsk unpend the north w\n",
      "================================================================================\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 2100: 1.641365 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 2200: 1.666501 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 2300: 1.638707 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 2400: 1.640073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 2500: 1.650863 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 2600: 1.637672 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 2700: 1.620755 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 2800: 1.619512 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 2900: 1.617127 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 3000: 1.633285 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "nherkory of preevese angels special depently rick begoyet three one seven each b \n",
      "jstic first to believed incrops biography and into or abodice orrence pents in th\n",
      "fyiyor specially for ursus rach one whur mine the black moncerondnerary is into e\n",
      "wrist drevels their more qarces force will v as coursee seemer college ors an ach\n",
      "uverse de by long in theu dejecinto s colary north on the front sert cention one \n",
      "================================================================================\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 3100: 1.607698 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 3200: 1.622719 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 3300: 1.619610 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 3400: 1.618937 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 3500: 1.607918 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 3600: 1.625909 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 3700: 1.593615 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 3800: 1.595226 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 3900: 1.585614 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 4000: 1.603693 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "================================================================================\n",
      " the nonatorica ideator again factors from human subjectners artic metribution an\n",
      "jgs to a three brities in secution add thous the octhoro haus the houser framembe\n",
      "rqualue to the seven founder hat in seen failler the puter one one nine nine six \n",
      "uven all have pated some for seven a manan be x math and is councoral the simmons\n",
      "x indical unks tok to v than donio land xi know writer to the of three through ex\n",
      "================================================================================\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 4100: 1.613718 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 4200: 1.597924 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 4300: 1.564994 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 4400: 1.587215 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 6.60\n",
      "Average loss at step 4500: 1.580288 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 4600: 1.583433 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 4700: 1.593230 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 4800: 1.586886 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 4900: 1.609750 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 5000: 1.618724 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.73\n",
      "================================================================================\n",
      "kkedally a lobelins of iiisment unionary potte suppread not from manics for mikee\n",
      "qmt of first to june zero neople estate to gextedus his this to a rendised are gr\n",
      "nnected the promous tre two chapification proviners carl can of firchough constil\n",
      "stripa east invensydhousamon cribino orited to compane were allism back or usebre\n",
      "vlrienciessor ein the worth imagreen for cle of text and no the standunically brk\n",
      "================================================================================\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 5100: 1.579907 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 5200: 1.587044 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 5300: 1.562284 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 5400: 1.556566 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 5500: 1.556941 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 5600: 1.542899 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 6.21\n",
      "Average loss at step 5700: 1.575472 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 6.21\n",
      "Average loss at step 5800: 1.562665 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 5900: 1.567995 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 6000: 1.528833 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.97\n",
      "================================================================================\n",
      "racty both the pulvy many and show referrisias major powers it vit vw and corated\n",
      "uffice of he were required commerds the tance for the morla societ encyclin in or\n",
      "jbe is moqne one as a the noted by does mass to as the turborese infrafficing spe\n",
      "ynatic or can film but the sold promic from the us of main of recodere februarca \n",
      "ybited in anoth grocks had claimon also be world with ordinal de hism in softv be\n",
      "================================================================================\n",
      "Validation set perplexity: 6.21\n",
      "Average loss at step 6100: 1.583996 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 6200: 1.577196 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 6300: 1.560125 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 6400: 1.580455 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 6500: 1.571360 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 6600: 1.566258 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 6700: 1.558975 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 6800: 1.571754 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 6.21\n",
      "Average loss at step 6900: 1.596151 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.21\n",
      "Average loss at step 7000: 1.579669 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "================================================================================\n",
      "philles one nine eight pino a third h strain mathening the established her the lo\n",
      "ues rular the class namedimater are the he ble forms commanged the as and the sta\n",
      "mjoche policial the however overstration naz meas i baration explagoods he is a f\n",
      "uch be john it from askil huall a centillloke of saltifasit and amudae linbaveni \n",
      "fkmt this of must of borm the was that enown bdcnoes bears and recently disbangin\n",
      "================================================================================\n",
      "Validation set perplexity: 6.19\n",
      "--- 103.78671097755432 seconds ---\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1): \n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:]) # Create the labels.\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))) # Calculate perplexity of batch.\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = [sample(random_distribution()), sample(random_distribution())]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input_i: \n",
    "                                                 feed_i for sample_input_i, feed_i in zip(sample_input, feed)})\n",
    "            feed.append(sample(prediction))\n",
    "            del feed[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input_i: \n",
    "                                                 feed_i for sample_input_i, feed_i in zip(sample_input, b)})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Introduce Dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce dropout at the input and increase the complexity in terms of embedding size and number of nodes in the LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 180)\n",
      "(64, 180)\n",
      "(64, 180)\n",
      "(64, 180)\n",
      "(64, 180)\n",
      "(64, 180)\n",
      "(64, 180)\n",
      "(64, 180)\n",
      "(64, 180)\n",
      "(64, 180)\n",
      "(64, 180)\n",
      "(64, 180)\n",
      "(64, 180)\n",
      "(64, 180)\n",
      "(64, 180)\n",
      "(64, 180)\n",
      "(64, 180)\n",
      "(64, 180)\n",
      "(64, 180)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 124\n",
    "embedding_size = 180\n",
    "dropout_rate = 0.7\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Embedding, now embedding a bigram input.\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size**2, embedding_size], -1.0, 1.0))\n",
    "    \n",
    "    # Parameters, adjust the inputs to be of size embedding_size instead of vocabulary_size:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    # Merge.\n",
    "    i_m = tf.concat([ix, fx, cx, ox], 1)\n",
    "    o_m = tf.concat([im, fm, cm, om], 1)\n",
    "    b_m = tf.concat([ib, fb, cb, ob], 1)\n",
    "  \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases. Output is still an probability ditribution over characters.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Every gate takes the new input (a one-hot-encoded word) -> output, previous output and a bias.\n",
    "    # We store the outputs and states across unrollings in saved_output and saved_state.\n",
    "\n",
    "    # The classifier then uses the output to predict a probablity distribution of the next character.\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        input_forget_update_out = tf.matmul(i, i_m) + tf.matmul(o, o_m) + b_m\n",
    "        inp, forg, update, out = tf.split(input_forget_update_out, 4, 1)\n",
    "        input_gate = tf.sigmoid(inp)\n",
    "        forget_gate = tf.sigmoid(forg)\n",
    "        output_gate = tf.sigmoid(out)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_temp = train_data[:num_unrollings]\n",
    "    train_inputs = [(train_data[i], train_data[i+1]) for i in range(len(train_temp)-1)]\n",
    "    train_labels = train_data[2:]\n",
    "    #print(len(train_inputs))\n",
    "    #print(len(train_labels))\n",
    "    \n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs: # For each unrolling we have an 64 new characters.\n",
    "        \"\"\"Given the new embed-coded input character, the previous state and the output of the previous LSTM cell, get the new\n",
    "        output and state. And then append the output to ouputs, since we're going to compare each output to the labels \n",
    "        stored in train_labels.\"\"\"\n",
    "        embed_i = tf.nn.embedding_lookup(embeddings, \n",
    "                            tf.argmax(i[0], axis=1) + vocabulary_size*tf.argmax(i[1], axis=1)) # Change input to LSTM to the embedding.\n",
    "        #print(embed_i.get_shape())\n",
    "        dropout_i = tf.nn.dropout(embed_i, dropout_rate) # Add dropout to input.\n",
    "        output, state = lstm_cell(dropout_i, output, state) \n",
    "        outputs.append(output) \n",
    "\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    # tf.control_dependencies ensures that we update saved_output and saved_state before performing the loss calculations.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        # tf.concat(x,0) merges the first dimension of x with it's second.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        #print(logits.get_shape())\n",
    "        #print(tf.concat(train_labels, 0).get_shape())\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))  # (gradient, value) tuple\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25) # Clip the gradients to avoid \"exploding gradient\"\n",
    "    optimizer = optimizer.apply_gradients( \n",
    "    zip(gradients, v), global_step=global_step) # Optimize with clipped gradients.\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = [tf.placeholder(tf.float32, shape=[1, vocabulary_size]) for _ in range(2)] # Bigram input.\n",
    "    #print(sample_input)\n",
    "    sample_inpit_embed = tf.nn.embedding_lookup(embeddings, \n",
    "            tf.argmax(sample_input[0], axis=1) +  vocabulary_size*tf.argmax(sample_input[1], axis=1)) # Embedded input\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes])) # Sample output.\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes])) # Sample saved state.\n",
    "    reset_sample_state = tf.group( # To clear the memory of network, at the start of every new sequence. \n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "    sample_inpit_embed, saved_sample_output, saved_sample_state) # Generate new output and state.\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), # Ensure variables updated.\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b)) # Make sample prediciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.361895 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.84\n",
      "================================================================================\n",
      "fbz m remeooy o v hgjrs n  me gmes u i slpotekapeeyley uirkes pdom czinj zo  u n \n",
      "eph vveeuiiem th u i strbes wh yts kzhgcg  f vcen emesileeisrwjzeoded eeaertn lfv\n",
      "ahzi n es stc ttesbfeteid et ioekf  se eg   koqdezjnv qs oep ikz   ne lc coeniyex\n",
      "ukl  zdtvqeeoteuaz n o j cpy q eweeoeeyks xs lzuco r vsea  ja  ieeudhsmteh tqh vv\n",
      "py keu eo l oea sle i  sc bekvep hslg unyebdwa eko cselbnt m d enrseeqoeekbo s ev\n",
      "================================================================================\n",
      "Validation set perplexity: 27.33\n",
      "Average loss at step 400: 2.114720 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.70\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 800: 1.869328 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 1200: 1.784729 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 1600: 1.760834 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 2000: 1.739408 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 2400: 1.722758 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 2800: 1.720449 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 3200: 1.701061 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 3600: 1.693603 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 4000: 1.692451 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "================================================================================\n",
      "vk example gency archina hiswan god relatic capacifice instaurally instories to b\n",
      "untal rencoorde mall mixis portnetal orst indulnaly smodern but optian people the\n",
      "zp casian e still of flower take often for zero zero zero zero zero zero zero zer\n",
      "go two example day redeprivertal for outs in rils if inquolose he is do becander \n",
      "jlock ould gas phosin his beachet that of the nique rash officially not it tapes \n",
      "================================================================================\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 4400: 1.681892 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 4800: 1.661515 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 5200: 1.653531 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 5600: 1.644324 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 6000: 1.645391 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 6400: 1.649296 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 6800: 1.645205 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 7200: 1.638123 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 7600: 1.645683 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 8000: 1.626173 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "================================================================================\n",
      "jv employal daska in hasurth life this alternal ill alwayle to books on forms own\n",
      " for and execular taged two zero zero zero four five four eight five zero kirely \n",
      "wn warn problem existed from the king the blue om the numbers prov carkever learc\n",
      "ament and what the university bratehad a can levid earlie used that void parsure \n",
      "fg invoman broke has arines of anti un in badith eight five nine eight four met a\n",
      "================================================================================\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 8400: 1.632687 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 8800: 1.661971 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 9200: 1.640562 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 9600: 1.641950 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 10000: 1.630258 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 10400: 1.652322 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 10800: 1.646190 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 11200: 1.663765 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 11600: 1.651866 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 12000: 1.664579 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.32\n",
      "================================================================================\n",
      "yzween outshiperman lival and koidea it books wherice to of the coing sign are ac\n",
      "mlord molouing emmight some government by referred a mean of this the name be not\n",
      "nically johh led for the emption to france to as at a trived aviok in agement of \n",
      "dn a clanues of guinated gack of the care of the eight passes telectrong instrume\n",
      "nun and latter also louised it was lance sone on nine three eight two zero zero z\n",
      "================================================================================\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 12400: 1.652283 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 12800: 1.658193 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 13200: 1.655097 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 13600: 1.641914 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 14000: 1.630753 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 14400: 1.673649 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 14800: 1.631365 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 15200: 1.656078 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 15600: 1.643415 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 16000: 1.647329 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "bxi notes hals syncceversificaniz is l usually an et is trolights aboy between ex\n",
      "nal losed by inesports indianaftered to t his was with they alphy some would hava\n",
      "gs harry of memberge acaded is internry plaffects and piccond has the first from \n",
      "de immerically limitary ch one nine proper one seven five two sevente nine six fo\n",
      "ecian was authors many dar six incple of conforms ango three now to often often f\n",
      "================================================================================\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 16400: 1.651915 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 16800: 1.629734 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 17200: 1.645133 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 17600: 1.649960 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.83\n",
      "Average loss at step 18000: 1.629276 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 18400: 1.639973 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 18800: 1.636504 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 19200: 1.647540 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 19600: 1.677715 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 20000: 1.651199 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "xds   ke which turn it ia the laking sorter the government his histories one one \n",
      "cp in shaped timily to eight and it bpick acquired and eight two spher presined a\n",
      "gqls to memberb one john oriesd instimunity gmented by up the broad and gdom betw\n",
      "es are form located at the bloadional place mare died to britian yourcan typice t\n",
      "ither dimates of the well is two nine most togethe restory a mirstronome inhearli\n",
      "================================================================================\n",
      "Validation set perplexity: 5.84\n",
      "--- 398.3056130409241 seconds ---\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "summary_frequency = 400\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1): \n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:]) # Create the labels.\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))) # Calculate perplexity of batch.\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = [sample(random_distribution()), sample(random_distribution())]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input_i: \n",
    "                                                 feed_i for sample_input_i, feed_i in zip(sample_input, feed)})\n",
    "            feed.append(sample(prediction))\n",
    "            del feed[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input_i: \n",
    "                                                 feed_i for sample_input_i, feed_i in zip(sample_input, b)})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified input/text8.zip\n",
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "def maybe_download(filename, expected_bytes, location):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, os.path.join(location,filename))\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "  \n",
    "filename = maybe_download('text8.zip', 31344016, 'input')\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'as', 'a', 'positive', 'label', 'by', 'self', 'defined', 'anarchists', 'the', 'word', 'anarchism', 'is', 'derived', 'from', 'the', 'greek', 'without', 'archons', 'ruler', 'chief', 'king', 'anarchism', 'as', 'a', 'political', 'philosophy', 'is', 'the', 'belief', 'that', 'rulers', 'are', 'unnecessary', 'and', 'should', 'be', 'abolished', 'although', 'there', 'are', 'differing']\n"
     ]
    }
   ],
   "source": [
    "data_words = text.split() # Split data into words and keep spaces.\n",
    "print(data_words[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207\n",
      " ['interpretations', 'of', 'what', 'this', 'means', 'anarchism', 'also', 'refers', 'to', 'related', 'social', 'movements', 'that', 'advocate', 'the', 'elimination', 'of', 'authoritarian', 'institutions', 'particularly', 'the', 'state', 'the', 'word', 'anarchy', 'as', 'most', 'anarchists', 'use', 'it', 'does', 'not', 'imply', 'chaos', 'nihilism', 'or', 'anomie', 'but', 'rather', 'a', 'harmonious', 'anti', 'authoritarian', 'society', 'in', 'place', 'of', 'what', 'are', 'regarded', 'as', 'authoritarian', 'political', 'structures', 'and', 'coercive', 'economic', 'institutions', 'anarchists', 'advocate', 'social', 'relations', 'based', 'upon']\n",
      "100\n",
      " ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'as', 'a', 'positive', 'label', 'by', 'self', 'defined', 'anarchists', 'the', 'word', 'anarchism', 'is', 'derived', 'from', 'the', 'greek', 'without', 'archons', 'ruler', 'chief', 'king', 'anarchism', 'as', 'a', 'political', 'philosophy', 'is', 'the', 'belief', 'that', 'rulers', 'are', 'unnecessary', 'and', 'should', 'be', 'abolished', 'although', 'there', 'are', 'differing']\n"
     ]
    }
   ],
   "source": [
    "valid_size = 100\n",
    "valid_words = data_words[:valid_size]\n",
    "train_words = data_words[valid_size:]\n",
    "train_size = len(data_words)\n",
    "print(str(train_size) + \"\\n\", train_words[:64])\n",
    "print(str(valid_size) + \"\\n\", valid_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deemed that is only such total or equivalent      ', 'i anarcho two the two when soviet follow genocide ', 'originally introduction the his torpparit zero    ', 'other westminster fled in gland in s dido of      ', 'charge it two that one and tago three circuits of ', 'if directly third ed r metal pope plagues life    ', 'keep increased three german english lattice to    ', 'five compose all the warming pakistani hollywood  ', 's council eight research produces ump the s of    ']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "sentance_length = 50 # How long a sentance will be.\n",
    "\n",
    "\n",
    "def generate_batch(data, batch_size):\n",
    "    batch = np.zeros(shape=(batch_size, sentance_length, vocabulary_size), dtype=np.float)\n",
    "    for j in range(batch_size):\n",
    "        i = 0\n",
    "        batch_j = list()\n",
    "        while i < sentance_length:\n",
    "            ind = np.random.randint(1,len(data))\n",
    "            randomword = data[ind]\n",
    "            if i + len(randomword) + 1 < sentance_length:\n",
    "                if i != 0: \n",
    "                    #batch_j.append(' ')\n",
    "                    #i += 1\n",
    "                    i += 1\n",
    "                batch_j.append(randomword)\n",
    "                i += len(randomword)\n",
    "                #print(i)\n",
    "            else:\n",
    "                if batch_j: break\n",
    "        for k,char in enumerate(' '.join(batch_j)):\n",
    "            batch[j, k, char2id(char)] = 1.0\n",
    "    return batch\n",
    "\n",
    "def batch2string(batch):\n",
    "    batch_string = list()\n",
    "    for bat in batch:\n",
    "        s = \"\"\n",
    "        for b in bat: \n",
    "            s= s + (id2char(np.argmax((b))))\n",
    "        batch_string.append(s)\n",
    "    return batch_string\n",
    "\n",
    "batch = generate_batch(train_words, batch_size)\n",
    "print(batch2string(batch)[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 24\n",
    "embedding_size = 100\n",
    "dropout_rate = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Embedding, now embedding a bigram input.\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    \n",
    "    # Parameters, adjust the inputs to be of size embedding_size instead of vocabulary_size:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    # Merge.\n",
    "    i_m = tf.concat([ix, fx, cx, ox], 1)\n",
    "    o_m = tf.concat([im, fm, cm, om], 1)\n",
    "    b_m = tf.concat([ib, fb, cb, ob], 1)\n",
    "  \n",
    "    # Parameters, adjust the inputs to be of size embedding_size instead of vocabulary_size:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix2 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    im2 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib2 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx2 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fm2 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb2 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx2 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cm2 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb2 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox2 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    om2 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob2 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    # Merge.\n",
    "    i_m2 = tf.concat([ix2, fx2, cx2, ox2], 1)\n",
    "    o_m2 = tf.concat([im2, fm2, cm2, om2], 1)\n",
    "    b_m2 = tf.concat([ib2, fb2, cb2, ob2], 1)\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases. Output is still an probability ditribution over characters.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Every gate takes the new input (a one-hot-encoded word) -> output, previous output and a bias.\n",
    "    # We store the outputs and states across unrollings in saved_output and saved_state.\n",
    "\n",
    "    # The classifier then uses the output to predict a probablity distribution of the next character.\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        input_forget_update_out = tf.matmul(i, i_m) + tf.matmul(o, o_m) + b_m\n",
    "        inp, forg, update, out = tf.split(input_forget_update_out, 4, 1)\n",
    "        input_gate = tf.sigmoid(inp)\n",
    "        forget_gate = tf.sigmoid(forg)\n",
    "        output_gate = tf.sigmoid(out)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    def lstm_cell_decode(i, o, state):\n",
    "        input_forget_update_out = tf.matmul(i, i_m2) + tf.matmul(o, o_m2) + b_m2\n",
    "        inp, forg, update, out = tf.split(input_forget_update_out, 4, 1)\n",
    "        input_gate = tf.sigmoid(inp)\n",
    "        forget_gate = tf.sigmoid(forg)\n",
    "        output_gate = tf.sigmoid(out)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(sentance_length):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data\n",
    "    train_labels = train_data[::-1]\n",
    "    #print(len(train_inputs))\n",
    "    #print(len(train_labels))\n",
    "    \n",
    "    # Encode\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs: # For each unrolling we have an 64 new characters.\n",
    "        \"\"\"Given the new embed-coded input character, the previous state and the output of the previous LSTM cell, get the new\n",
    "        output and state. And then append the output to ouputs, since we're going to compare each output to the labels \n",
    "        stored in train_labels.\"\"\"\n",
    "        #print(i.get_shape())\n",
    "        embed_i = tf.nn.embedding_lookup(embeddings, \n",
    "                            tf.argmax(i, axis=1))\n",
    "        #dropout_i = tf.nn.dropout(embed_i, dropout_rate) # Add dropout to input.\n",
    "        #print(embed_i.get_shape())\n",
    "        #print(output.get_shape())\n",
    "        output, state = lstm_cell(embed_i, output, state) \n",
    "\n",
    "    for i in train_labels:\n",
    "        output, state = lstm_cell_decode(output, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    # tf.control_dependencies ensures that we update saved_output and saved_state before performing the loss calculations.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        # tf.concat(x,0) merges the first dimension of x with it's second.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        #print(logits.get_shape())\n",
    "        #print(tf.concat(train_labels, 0).get_shape())\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))  # (gradient, value) tuple\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25) # Clip the gradients to avoid \"exploding gradient\"\n",
    "    optimizer = optimizer.apply_gradients( \n",
    "    zip(gradients, v), global_step=global_step) # Optimize with clipped gradients.\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = [tf.placeholder(tf.float32, shape=[1, vocabulary_size]) for _ in range(sentance_length)] \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes])) # Sample output.\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes])) # Sample saved state.\n",
    "    reset_sample_state = tf.group( \n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    \n",
    "    sample_outputs = list()\n",
    "    sample_output = saved_sample_output\n",
    "    sample_state = saved_sample_state\n",
    "    for i in sample_input:\n",
    "        embed_i = tf.nn.embedding_lookup(embeddings, \n",
    "            tf.argmax(i, axis=1))\n",
    "        sample_output, sample_state = lstm_cell(embed_i, sample_output, sample_state)\n",
    "    \n",
    "    for i in sample_input:\n",
    "        sample_output, sample_state = lstm_cell_decode(sample_output, sample_output, sample_state)\n",
    "        sample_outputs.append(output)\n",
    "        \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), # Ensure variables updated.\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        log = tf.nn.xw_plus_b(tf.concat(sample_outputs, 0), w, b)\n",
    "        sample_prediction = tf.nn.softmax(log)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.034232 learning rate: 10.000000\n",
      "Minibatch perplexity: 772332471246575071240784494286477140677903726883612044585766748160.00\n",
      "['norway to patterns whereas zero time by           ', 'dialects other nine a red and area marxism colony ', 'nutmeg a also three s ships she academics gleam   ']\n",
      "['dwsddddfsddddddddddkdddddddddddddddddddddddddddddd', 'ddhddbddddwdddvwvvvvjfvvvvvvvvvvvbvvvrvvvvvrvvvvvv', 'vvvvvvvvvvvvvvvvevvrvvvvwrvvdwdddddhdddddddddddbdd']\n",
      "Average loss at step 100: 2.693113 learning rate: 10.000000\n",
      "Minibatch perplexity: 149123143524705647442546672025967691269368044347814857146368.00\n",
      "['anthropology the one and hapalinae kind is large  ', 'answer issues from property habitable action duc  ', 'a eight one a berlin of watering as oils way van  ']\n",
      "['eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee', 'eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee', 'eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee']\n",
      "Average loss at step 200: 2.639030 learning rate: 10.000000\n",
      "Minibatch perplexity: 592876963529201935447414989407011164982997404901798051840000.00\n",
      "['no two two is maitland hangman number cell may    ', 'specific bce five time in who debugging coup s    ', 'in zone th selection wild developments five books ']\n",
      "['nnnnnnnnnnnnnnnnnnnnnannnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnnnnnnnnnnnnannnnnnnnnnnnanaannennnnnnan', 'nnnnnnnnnnennnnnnnnnnnnnnnnnnnnnnnnnanennnnnnnnnna']\n",
      "Average loss at step 300: 2.618443 learning rate: 10.000000\n",
      "Minibatch perplexity: 128408091613754694588114724036204124657335543165460368326656.00\n",
      "['a seismographs to those cornered light golem one  ', 'its dining most vestal tables future front        ', 'was nations appreance five phonemic the           ']\n",
      "['ennnnnnnnnnnennnnnnnnnnnnnnnnennnnnnnnnnnennnnnnnn', 'nnnnnnennneennennnnnnnnnnnennnnnnnnnennnnenennnnnn', 'nnnnnennnnnnnnnnnnnnenneeennennnnnnnnnnnennnennnnn']\n",
      "Average loss at step 400: 2.596394 learning rate: 10.000000\n",
      "Minibatch perplexity: 1443944836368334210687748016336579320513687670657290864689152.00\n",
      "['structures preponderance three entire included    ', 'blackbody pretenses to such and to one executive  ', 'two this nine the other all to and robbery of the ']\n",
      "['rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrorrrrrrrrrrrrrrrrr', 'rrrrrrrrrrrrrrrrerrrrrrrrerrrrrrrrrrrrrrrrrrrrtrre', 'rrerrrrrrrrrrrrrrorrrtorrrrrreeretrterrerrrerrrrrr']\n",
      "Average loss at step 500: 2.567440 learning rate: 10.000000\n",
      "Minibatch perplexity: 51674117754279810891512502654131129150185595911713650500435968.00\n",
      "['as traits divine nine artistic and a of someone   ', 'with as forces to itself gold and sometimes       ', 'an edgar station in pronounced was and projected  ']\n",
      "['nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnennennnnnnnnnnnnnnnnnn', 'ennnnnnnnnnnnnnnananennnnnnnnnennnnnnnnnnnhnnennnn']\n",
      "Average loss at step 600: 2.545009 learning rate: 10.000000\n",
      "Minibatch perplexity: 2966648728509209904868654574456318598349874108660996452122624.00\n",
      "['air polar stanley american be sovereignty         ', 'zero to new with to stories many ebazake          ', 'journalist such n before one the such clue the    ']\n",
      "['nnnnnnnnnnnnnnnnenennnnnnnnnnennnnnnnnnnnnnnnnnnen', 'nnnnnnnnnnnnnnnnnnnnnennnnnnnnenennnnnnnnnnennnnne', 'inennnnnnnneennntnnnneninnnnnnnnnneennnennnnhnhnnn']\n",
      "Average loss at step 700: 2.570566 learning rate: 10.000000\n",
      "Minibatch perplexity: 6246475488480063108265158954569334544509956866405224979365888.00\n",
      "['appendix by war to called however and world many  ', 'in parts conditions times continuous health eight ', 'the of czech literary party joe s to spanish five ']\n",
      "['nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnnnnnnnnnnnnannnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnhnnenna nnnnnnnnnnnn']\n",
      "Average loss at step 800: 2.547316 learning rate: 10.000000\n",
      "Minibatch perplexity: 64893909325043403781429749816676543710694215045849322541285376.00\n",
      "['zero graeco nine a mac nine appearance benjamin   ', 'and in one an kesava kennedy principle            ', 'better eight largest cultural following           ']\n",
      "['tttttttttttttttttttttttttttttttttttttttttttttttttt', 'tttttttttttttttttrtttttttttttttttttttttttttttttttt', 'tttttttttttttttttttttttttttttttetttttttta ttttttt ']\n",
      "Average loss at step 900: 2.528211 learning rate: 10.000000\n",
      "Minibatch perplexity: 56324991139705530540134687761185989084047028631227213709049856.00\n",
      "['rare government syracuse and is teborg jalal      ', 'mu arena who typically with media technology      ', 'the formed yet again n nine sweets s to nine      ']\n",
      "['eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee', 'eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeoeeeeeeeeeeeeeeeeaee', 'eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeoeeeea']\n",
      "Average loss at step 1000: 2.517459 learning rate: 10.000000\n",
      "Minibatch perplexity: 1364007850329507023094115509588610561619893906082607918418493440.00\n",
      "['located requirements is archipelago some four     ', 'as of question was plant used from they nine nine ', 'two wings kingdom evolution ron zero uzbeks for   ']\n",
      "['nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnnnnnnnannnnnnnnannhnnnhnnnnnnnnnnnennnn']\n",
      "Average loss at step 1100: 2.521264 learning rate: 10.000000\n",
      "Minibatch perplexity: 57189901661038651974177349757842510001093146137269681694703616.00\n",
      "['every an four ammoniac national made the          ', 'by in working in appeal involved thus the is      ', 'costs jewish for p adopted asimov klimt holds for ']\n",
      "['nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnnnnnnnnnnannnnnnnnnnannnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnonnnannnninnnnnaennn']\n",
      "Average loss at step 1200: 2.508362 learning rate: 10.000000\n",
      "Minibatch perplexity: 119451148121858913240470964770670892528851849477742968568807424.00\n",
      "['moscow amplitude asia to blocked the sugar the    ', 'the into zero never a results in one five was of  ', 'for at with colorado kennedy magnetic of six      ']\n",
      "['eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeoeeeeeeeeeeeeeeeeee', 'eeeeeeeeeeeeeeooooeootooeoaooooeooooeoeeooooeoooto', 'atooooeoooooooooooooootoeeoeooooeeoiooeoaooooeoooo']\n",
      "Average loss at step 1300: 2.492218 learning rate: 10.000000\n",
      "Minibatch perplexity: 8106005774086441772885867318529426600589113213085275762976620544.00\n",
      "['only narrating aye not notice overwhelmingly      ', 'married called nine caesar inspection official    ', 'bichvinta hard century seven tests inaccurate     ']\n",
      "['oooooooooooooooooooooooooootoooooooooooooaoooooooo', 'oooooooooooooonnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnont', 'nnnonannnnnnnnnnnnnnnnnnnnnnnnnnnnnennnnnnnnnnnnnn']\n",
      "Average loss at step 1400: 2.490467 learning rate: 10.000000\n",
      "Minibatch perplexity: 4296656085836050998077820685261501255994687860592512186521223168.00\n",
      "['ultimate and zero as empire zero usa body been    ', 'of he upon about command in to effect of ts       ', 'film an five nine communication other             ']\n",
      "['eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee', 'eeeeeeeeeeeeeeeeeeeeaeeeeeeeeneeeeeeeeeeeeeeeeeeee', 'eneeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaeeaeeeeiieeee e']\n",
      "Average loss at step 1500: 2.490074 learning rate: 10.000000\n",
      "Minibatch perplexity: 1021527631905381050026499077328451760516233028540277654514827264.00\n",
      "['starts is convinced openoffice the treats quality ', 'and placed independent still as new rolling six   ', 'find members spend water the of always arrivals   ']\n",
      "['oooooooooooooooooooooooooooooooonooooooooooooooooo', 'oooooooooooooonnnnnnonnnnnnnonnnnonnnnnnnnnnnnannn', 'nnnnnnnnnnnnnnnnnnnnnnnnnnnninnnnn nnnnnnnonnnnonn']\n",
      "Average loss at step 1600: 2.478153 learning rate: 10.000000\n",
      "Minibatch perplexity: 2171840255342811829233049756648487850785647220440974470257049600.00\n",
      "['monomorphism ruth height female such german nine  ', 'many of total linear one list round in class nine ', 'ever last one parliamentary a influence family    ']\n",
      "['oooooooooooooooooooooooooooooooooooooooooooooooooo', 'oooooooooooooooroooaooorooooooooaooooooooooooooooo', 'ooooorroooooooorooooorooooorreoooaoooeooroooooaoor']\n",
      "Average loss at step 1700: 2.473769 learning rate: 10.000000\n",
      "Minibatch perplexity: 5395647837609119241603515062475595031289331502429835221610266624.00\n",
      "['zero ais the jezebel the nearby to of in          ', 'and disclosures ammonium cracks only keyboard in  ', 'production after group is zero are of in and of   ']\n",
      "['oooooooooooooooooooooooooooooooooooooooooooooooooo', 'onooonoooooooonnnnnnnnnnnnnnnnnnnnnnnnnnnnonnnnnnn', 'nnnnnnnnnnnnnnnannnannnnnnnnnnnnnnnnnnnnneenannnnn']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 1800: 2.465129 learning rate: 10.000000\n",
      "Minibatch perplexity: 46166734021184222105251693950545868567880522896456734466891579392.00\n",
      "['veterans of palatalized then in is to rahman two  ', 'in damaged to maker two for in re atheist nine if ', 'eight one the available two the the babe zero     ']\n",
      "['ssssssssssssssssssssssssssssssssssssessssssssssses', 'ssssssssssssessssssssssssssssssssssssssssssssassss', 'esssssssssssesssssssssssssessisassnssssseaissssass']\n",
      "Average loss at step 1900: 2.466820 learning rate: 10.000000\n",
      "Minibatch perplexity: 53424559668538273408879852028727132974304461335271449342697799680.00\n",
      "['lunar years and robotic of some believed his a    ', 'highly islam his with etching in were of great    ', 'is one saddles battle not eight to its seven that ']\n",
      "['oooooooooooooooooooooooooooooooooooooooooooooooooo', 'ooooooooooooooooaooottoooooooooootoooooooooooosoos', 'ooaoosooootoosootoooooooooootoaoosatttooootooottos']\n",
      "Average loss at step 2000: 2.460286 learning rate: 10.000000\n",
      "Minibatch perplexity: 103887167715057302199563991012005818359658577490069235161278447616.00\n",
      "['in source of prefix liquid force identity bigsby  ', 'american abnormal one s care come water it that   ', 'pol of in states the of if typical use zapruder   ']\n",
      "['ooooooooooooooonoooooooooooooooooooooooooooooooooo', 'oooooooooooooooooooooooooooooanoooooooooooooonoooo', 'nnoooonnoooooooooonoooooooooonoooooooooooonaaonooo']\n",
      "Average loss at step 2100: 2.459302 learning rate: 10.000000\n",
      "Minibatch perplexity: 2801591670775347615874096792406445278515999652159036601086553096192.00\n",
      "['citadel assemblage class zero and judicial from   ', 'hickory zero into of first heretic confessed one  ', 'the archaeology once in indeed they the to the    ']\n",
      "['eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee', 'eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaeeeeeeeeeeeeaeeeee', 'eeeeeeeeeeeeeeeaeeeeeeeeeeeeeeeeeheeeeeeeeeeeaeeee']\n",
      "Average loss at step 2200: 2.445894 learning rate: 10.000000\n",
      "Minibatch perplexity: 36533907669639301705071863088607774421672337488234136788589871104.00\n",
      "['discussion pale of in about the s tyler while     ', 'agriculture order members the the emperor restore ', 'on concludes stitching the xtmar was six lupulus  ']\n",
      "['nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nannnnnnnnnnnnnnnnnnnnnnnnnnnennenannnnnnnannnnnnn']\n",
      "Average loss at step 2300: 2.446948 learning rate: 10.000000\n",
      "Minibatch perplexity: 112057668222574021816240580139431793692228690421145991843590176768.00\n",
      "['attendance used declared mhz religion sweden      ', 'baseball are of a failures markets western        ', 'franchise sub passed t charlotte twin the that an ']\n",
      "['eedeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee', 'eeeeeeeeeeeeeettattttttt ttttttetttttttttttttttttt', 'ttetttttttttttttttttttttttttttatttattt tttttthttat']\n",
      "Average loss at step 2400: 2.443363 learning rate: 10.000000\n",
      "Minibatch perplexity: 379264194298562336805425000730117062582571956480373075083121393664.00\n",
      "['fact le nine turned simplex features channel      ', 'is remote are in his distinguish of zero one in   ', 'regular updated jorge statistically a north one   ']\n",
      "['oooosooossoooooooooooooooooosoooooooosossooooosooo', 'ooosooosssoooossssasssinssssssssssssssssssesssssss', 'ssssasssssessssssasssssessssssssaissiessnsssssssss']\n",
      "Average loss at step 2500: 2.445308 learning rate: 10.000000\n",
      "Minibatch perplexity: 899181066656254010687790722553792305396220927467780066945494155264.00\n",
      "['center one it total five some of below the and    ', 'time consists raphidae the follow of for six      ', 'soccer subgroup these called with move variations ']\n",
      "['hhhhhhhhhhhhhhhhhhhnhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh', 'hhnhhhhhhhhhhhnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnennnnnnnnnnnnann']\n",
      "Average loss at step 2600: 2.446604 learning rate: 10.000000\n",
      "Minibatch perplexity: 3374181606748617920222875753510646291432206896774099239062994944.00\n",
      "['also repeated bishops war of the eight san uk     ', 'adopt completely during five and would he         ', 'zero heroes two valet and in developer upstairs   ']\n",
      "['oooooooooooooooooooooooooooooooooooooooooooooooooo', 'ooooooooooooooooonooooooooooonooonoonooooooooooooo', 'nooooooooooooooonooooonnooooooonoooononooonenooeno']\n",
      "Average loss at step 2700: 2.454736 learning rate: 10.000000\n",
      "Minibatch perplexity: 39562310609537008342025413224490552810318386892790681940004962304.00\n",
      "['received who it nine hybrid of depends by the had ', 'the unacceptable admitted remains also custody    ', 'contemporaries from the the hall ex to aimed nine ']\n",
      "['nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnninnnnnnnnnnnnnnnnnnnnnnnnanennnnnnnnnnnnnnnnnnn']\n",
      "Average loss at step 2800: 2.439738 learning rate: 10.000000\n",
      "Minibatch perplexity: 14384115553541590712031323054038398633834088807358755413707395694592.00\n",
      "['also aardvarks with city marvel between judicious ', 'one choices like four nine then leaders paper     ', 'nine seven amerikaner a and short are ancestors   ']\n",
      "['nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnnnnnnannnnnnnnnnnnnnnnnnnnennnnnhnnnnnn']\n",
      "Average loss at step 2900: 2.441907 learning rate: 10.000000\n",
      "Minibatch perplexity: 26910251385810708337914606351992828524981720234653291603487499485184.00\n",
      "['government which places b and few the principle   ', 'the of without org to nine have meaning earliest  ', 'still opponents with as in that the all nine      ']\n",
      "['nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnaennnnnnnennnnnnnn']\n",
      "Average loss at step 3000: 2.451820 learning rate: 10.000000\n",
      "Minibatch perplexity: 876476859829777964099490526849548640094163603062680513754066583552.00\n",
      "['one nine nine was found point or one locomotion   ', 'a was most presence until that this ii with       ', 'scene still eight such with of players variol     ']\n",
      "['nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnonnnnnnnnnnnnnnnnnnnnnnnnnnannnennnnnnnnan']\n",
      "Average loss at step 3100: 2.451002 learning rate: 10.000000\n",
      "Minibatch perplexity: 420804627104780211089043929757785041305664912041263155364252090368.00\n",
      "['meaning by palmyra unusual make course one        ', 'before curtis in are constructing first work      ', 'continue been referendum zero the victorian with  ']\n",
      "['hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh', 'hhhhhhhhhhhhhhnnnnnnnnnnnnnnnnnnnnnnnnnnannnnnnnnn', 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnannnnnnnnnnnnnnnnne']\n",
      "Average loss at step 3200: 2.441809 learning rate: 10.000000\n",
      "Minibatch perplexity: 1081280993901643553243321781686505635657698579617258435929954058240.00\n",
      "['one citizen sins nine three in develop mainly     ', 'postal more smith five it one integral            ', 'tendency variables passions an with more yr       ']\n",
      "['tttttttttttttttttttttttttttttttttttttttttttttttttt', 'ttttttttttttttttttttnttttttttttttttttttttttttttttt', 'tttttnttttttttttttttttttttntttttttettttttttntttttt']\n",
      "Average loss at step 3300: 2.440036 learning rate: 10.000000\n",
      "Minibatch perplexity: 569556532889937725732935347496743385482347340163069795050159341568.00\n",
      "['and zero be with however first f name with four   ', 'contrast development when del assassin sales      ', 'before zero fungi repopulated black give milky    ']\n",
      "['eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee', 'eeeeeeeeeeeeeessssssssssssssssseissssssssssssssses', 'ssssssssssessssssessssssssssssssssssssssssssseiess']\n",
      "Average loss at step 3400: 2.434870 learning rate: 10.000000\n",
      "Minibatch perplexity: 1319695813575664425679442116339803187650169313940861475302062161920.00\n",
      "['relatively the port by canon they for the         ', 'nkvd five one double or died eight don an         ', 'hat yet preview can the services the majority for ']\n",
      "['eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaeeeeeeeee', 'eeeeeeeeeeeeeelllelelllllelllllllleellllelelleello', 'elllalellleollloelellllllellllfelellallellllllllee']\n",
      "Average loss at step 3500: 2.439918 learning rate: 10.000000\n",
      "Minibatch perplexity: 41508945302437406084738973317058091152293420389158696820019843563520.00\n",
      "['axis perk if wikicities track and throughout two  ', 'universal entertainment parties th colbert teams  ', 'ancestors after series the of two city is all z   ']\n",
      "['eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee', 'eeeeeeeeeeeeeennnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnne', 'nnnnnnnnnnnnennnnnnnnnnnnnnnnnnnnnnennnnneninnennn']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 3600: 2.470948 learning rate: 10.000000\n",
      "Minibatch perplexity: 9321136119417829648803873080190439881390636629602861025005013322630471300423047947747328.00\n",
      "['sin year road they von the people language        ', 'sketch presses eight the queen carriers these     ', 'became draft appear zero elegy statements french  ']\n",
      "['nnnnnnnnnnnnnnnnnnnnnannnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnannaaaaaanaaaaaaaaaaaananaaanaaaaaanaaa', 'aaanaaaaaaaannaaaaaaaannnnaaaaaaaanaaaaaaaaaaaanan']\n",
      "Average loss at step 3700: 19.055624 learning rate: 10.000000\n",
      "Minibatch perplexity: 13630274316923911372842946044956160347320710883384877906806021439256095027921388907918594242007513122279532241717454646864827558088055096123498237782888571595654788607493641197666678325569402048820416477060197615729912984675659407611244273252137075412142626177024.00\n",
      "['the joined stations of his as visual priests      ', 'alias on of in is cities this bonavista opposed   ', 'of visible there european the have eight broken   ']\n",
      "['eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee', 'eeeeeeeeeeeeeennnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn']\n",
      "Average loss at step 3800: 21.229569 learning rate: 10.000000\n",
      "Minibatch perplexity: inf\n",
      "['but three olympic european royal heartbreaking    ', 'hours institute who process over see of meeting   ', 'then and but science holidays cease bound of a    ']\n",
      "['nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn', 'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Tranheden/anaconda/lib/python3.5/site-packages/ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 3900: 21.783706 learning rate: 10.000000\n",
      "Minibatch perplexity: inf\n",
      "['civil anchor ring has most a provincial absolute  ', 'beverage through now theme abalone running        ', 'corporate arms became near at unitary essays non  ']\n",
      "['tttttttttttttttttttttttttttttttttttttttttttttttttt', 'tttttttttttttttttttttttttttttttttttttttttttttttttt', 'tttttttttttttttttttttttttttttttttttttttttttttttttt']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-393-ca69bd8f74cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     _, l, predictions, lr = session.run(\n\u001b[0;32m---> 15\u001b[0;31m       [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmean_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msummary_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tranheden/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tranheden/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tranheden/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/Tranheden/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tranheden/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch = generate_batch(train_words, batch_size)\n",
    "    feed_dict = dict()\n",
    "    for i in range(sentance_length): \n",
    "      feed_dict[train_data[i]] = batch[:,i,:]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = batch # Create the labels.\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions.reshape([labels.shape[0],labels.shape[1],labels.shape[2]]), labels)))) # Calculate perplexity of batch.\n",
    "      # Measure validation set perplexity.\n",
    "      print(batch2string(labels)[:3])\n",
    "      print(batch2string(predictions.reshape([labels.shape[0],labels.shape[1],labels.shape[2]])[:3]))\n",
    "      \"\"\"\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      b = generate_batch(valid_words, 1)\n",
    "      bb = np.reshape(b[0,:,:],[50,1,27])\n",
    "      d = {sample_input_i: \n",
    "          feed_i for sample_input_i, feed_i in zip(sample_input, bb)}\n",
    "      #print(d)\n",
    "      predictions = sample_prediction.eval(feed_dict = d)      \n",
    "      valid_logprob = valid_logprob + logprob(predictions, b)\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "      for i in predictions:\n",
    "            print(np.shape(i))\n",
    "      \"\"\"\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
